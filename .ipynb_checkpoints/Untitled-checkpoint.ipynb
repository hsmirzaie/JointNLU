{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def load_data(path):\n",
    "    \n",
    "    data_raw = open(path,\"r\").readlines()\n",
    "    data_raw = [t[:-1] for t in data_raw]\n",
    "    data_raw = [[t.split(\"\\t\")[0].split(\" \"),t.split(\"\\t\")[1].split(\" \")[:-1],t.split(\"\\t\")[1].split(\" \")[-1]] for t in data_raw]\n",
    "    data_raw = [[t[0][1:-1],t[1][1:],t[2]] for t in data_raw]\n",
    "    \n",
    "    return data_raw\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = list(map(lambda w: to_ix[w] if w in to_ix.keys() else to_ix['<UNK>'], seq))\n",
    "    tensor = Variable(torch.LongTensor(idxs))\n",
    "    return tensor\n",
    "\n",
    "def process_data(data_raw, LENGTH):\n",
    "        \n",
    "    seq_input, seq_slots, intents = list(zip(*data_raw))\n",
    "\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    \n",
    "    vocab = set(flatten(seq_input))\n",
    "    slot_tag = set(flatten(seq_slots))\n",
    "    intent_tag = set(intents)\n",
    "    \n",
    "    padded_input = []\n",
    "    padded_slots = []\n",
    "    \n",
    "    for i in range(len(seq_input)):\n",
    "        temp = seq_input[i].copy()\n",
    "        if len(temp)<LENGTH:\n",
    "            temp.append('<EOS>')\n",
    "            while len(temp)<LENGTH:\n",
    "                temp.append('<PAD>')\n",
    "        else:\n",
    "            temp = temp[:LENGTH]\n",
    "            temp[-1]='<EOS>'\n",
    "        padded_input.append(temp)\n",
    "    \n",
    "        temp = seq_slots[i].copy()\n",
    "        if len(temp)<LENGTH:\n",
    "            while len(temp)<LENGTH:\n",
    "                temp.append('<PAD>')\n",
    "        else:\n",
    "            temp = temp[:LENGTH]\n",
    "            temp[-1]='<EOS>'\n",
    "        padded_slots.append(temp)\n",
    "    \n",
    "    \n",
    "    \n",
    "    word2index = {'<PAD>': 0, '<UNK>':1,'<SOS>':2,'<EOS>':3}\n",
    "    for token in vocab:\n",
    "        if token not in word2index.keys():\n",
    "            word2index[token]=len(word2index)\n",
    "\n",
    "    \n",
    "    slot2index = {'<PAD>' : 0, '<UNK>':1}\n",
    "    for tag in slot_tag:\n",
    "        if tag not in slot2index.keys():\n",
    "            slot2index[tag] = len(slot2index)\n",
    "            \n",
    "            \n",
    "    intent2index={'<UNK>': 0}\n",
    "    for ii in intent_tag:\n",
    "        if ii not in intent2index.keys():\n",
    "            intent2index[ii] = len(intent2index)\n",
    "            \n",
    "    data_padded = list(zip(padded_input,padded_slots, intents))\n",
    "    \n",
    "    data_processed = []\n",
    "    \n",
    "    for d in data_padded:\n",
    "        temp = prepare_sequence(d[0],word2index)\n",
    "        temp = temp.view(1,-1)\n",
    "\n",
    "        temp2 = prepare_sequence(d[1],slot2index)\n",
    "        temp2 = temp2.view(1,-1)\n",
    "        \n",
    "        temp3 = Variable(torch.LongTensor([intent2index[d[2]] if d[2] in intent2index.keys() else intent2index['<UNK>']]))\n",
    "        \n",
    "        data_processed.append((temp,temp2,temp3))\n",
    "       \n",
    "    return data_processed, word2index, slot2index, intent2index\n",
    "    \n",
    "    \n",
    "def test_process(test_raw, word2index, slot2index, intent2index, LENGTH):\n",
    "    \n",
    "    seq_input, seq_slots, intents = list(zip(*test_raw))\n",
    "\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    \n",
    "    padded_input = []\n",
    "    padded_slots = []\n",
    "    \n",
    "    for i in range(len(seq_input)):\n",
    "        temp = seq_input[i].copy()\n",
    "        if len(temp)<LENGTH:\n",
    "            temp.append('<EOS>')\n",
    "            while len(temp)<LENGTH:\n",
    "                temp.append('<PAD>')\n",
    "        else:\n",
    "            temp = temp[:LENGTH]\n",
    "            temp[-1]='<EOS>'\n",
    "        padded_input.append(temp)\n",
    "    \n",
    "        temp = seq_slots[i].copy()\n",
    "        if len(temp)<LENGTH:\n",
    "            while len(temp)<LENGTH:\n",
    "                temp.append('<PAD>')\n",
    "        else:\n",
    "            temp = temp[:LENGTH]\n",
    "            temp[-1]='<EOS>'\n",
    "        padded_slots.append(temp)\n",
    "        \n",
    "    data_padded = list(zip(padded_input,padded_slots, intents))\n",
    "    \n",
    "    data_processed = []\n",
    "    \n",
    "    for d in data_padded:\n",
    "        temp = prepare_sequence(d[0],word2index)\n",
    "        temp = temp.view(1,-1)\n",
    "\n",
    "        temp2 = prepare_sequence(d[1],slot2index)\n",
    "        temp2 = temp2.view(1,-1)\n",
    "\n",
    "        temp3 = Variable(torch.LongTensor([intent2index[d[2]] if d[2] in intent2index.keys() else intent2index['<UNK>']]))\n",
    "        \n",
    "        data_processed.append((temp,temp2,temp3))\n",
    "        \n",
    "    return data_processed\n",
    "\n",
    "    \n",
    "def getBatch(batch_size,data, Shuffle = True):\n",
    "    \n",
    "    if Shuffle:\n",
    "        random.shuffle(data)\n",
    "    sindex=0\n",
    "    eindex=batch_size\n",
    "    while eindex < len(data):\n",
    "        batch = data[sindex:eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex+batch_size\n",
    "        sindex = temp\n",
    "        \n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = load_data(\"dataset/atis-2.train.w-intent.iob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed, word2index, slot2index, intent2index = process_data(train_raw, 50)\n",
    "index2slot = {v:k for k,v in slot2index.items()}\n",
    "index2intent = {v:k for k,v in intent2index.items()}\n",
    "index2word = {v:k for k,v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[62, 62, 62, 62, 62, 48, 62, 57, 56, 27,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_processed[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[417, 669, 666, 262, 503, 390, 666, 734, 200, 487,   3,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_processed[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'want',\n",
       " 'to',\n",
       " 'fly',\n",
       " 'from',\n",
       " 'baltimore',\n",
       " 'to',\n",
       " 'dallas',\n",
       " 'round',\n",
       " 'trip']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<PAD>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2slot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from model import Encoder,Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE=0.001\n",
    "EMBEDDING_SIZE=64\n",
    "HIDDEN_SIZE=64\n",
    "BATCH_SIZE=16\n",
    "LENGTH=60\n",
    "EPOCHS=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(len(word2index),EMBEDDING_SIZE,HIDDEN_SIZE)\n",
    "decoder = Decoder(len(slot2index),len(intent2index),len(slot2index)//3,HIDDEN_SIZE*2, dropout_p=0.8)\n",
    "\n",
    "encoder.init_weights()\n",
    "decoder.init_weights()\n",
    "\n",
    "loss_function_1 = nn.CrossEntropyLoss()\n",
    "loss_function_2 = nn.CrossEntropyLoss()\n",
    "enc_optim= optim.Adam(encoder.parameters(), lr=LEARNING_RATE)\n",
    "dec_optim = optim.Adam(decoder.parameters(),lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.dropout_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ho3in\\Anaconda3\\envs\\Joint_Intent_Slot\\model.py:105: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = F.softmax(attn_energies) # B,T\n",
      "C:\\Users\\Ho3in\\Anaconda3\\envs\\Joint_Intent_Slot\\model.py:160: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmaxed = F.log_softmax(score)\n",
      "C:\\Users\\Ho3in\\Anaconda3\\envs\\Deep-Learning-Ng\\lib\\site-packages\\ipykernel_launcher.py:50: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "C:\\Users\\Ho3in\\Anaconda3\\envs\\Deep-Learning-Ng\\lib\\site-packages\\ipykernel_launcher.py:51: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0  batch 0  :  7.9533896\n",
      "EPOCH 0  batch 100  :  2.5800538\n",
      "EPOCH 0  batch 200  :  1.2751521\n",
      "EPOCH 1  batch 0  :  0.44508877\n",
      "EPOCH 1  batch 100  :  0.95859885\n",
      "EPOCH 1  batch 200  :  0.9269259\n",
      "EPOCH 2  batch 0  :  0.87939763\n",
      "EPOCH 2  batch 100  :  0.7284668\n",
      "EPOCH 2  batch 200  :  0.7407963\n",
      "EPOCH 3  batch 0  :  0.55110496\n",
      "EPOCH 3  batch 100  :  0.55501133\n",
      "EPOCH 3  batch 200  :  0.5003172\n",
      "EPOCH 4  batch 0  :  0.14790893\n",
      "EPOCH 4  batch 100  :  0.4011585\n",
      "EPOCH 4  batch 200  :  0.39508682\n"
     ]
    }
   ],
   "source": [
    "intent_acc = []\n",
    "slot_acc = []\n",
    "for epoch in range(EPOCHS):\n",
    "    losses=[]\n",
    "    intent_truly_labeled = 0\n",
    "    intent_mislabeled = 0\n",
    "    slot_truly_labeled = 0\n",
    "    slot_mislabeled = 0\n",
    "    for i, batch in enumerate(getBatch(BATCH_SIZE,train_processed)):\n",
    "        x,y_1,y_2 = zip(*batch)\n",
    "        x = torch.cat(x)\n",
    "        slot_target = torch.cat(y_1)\n",
    "        intent_target = torch.cat(y_2)\n",
    "        x_mask = torch.cat([Variable(torch.BoolTensor(tuple(map(lambda s: s ==0, t.data)))) for t in x]).view(BATCH_SIZE,-1)\n",
    "        y_1_mask = torch.cat([Variable(torch.BoolTensor(tuple(map(lambda s: s ==0, t.data)))) for t in slot_target]).view(BATCH_SIZE,-1)\n",
    " \n",
    "        encoder.zero_grad()\n",
    "        decoder.zero_grad()\n",
    "\n",
    "        output, hidden_c = encoder(x,x_mask) # hidden_c : last hidden state of encoder to start decoder\n",
    "        \n",
    "        start_decode = Variable(torch.LongTensor([[word2index['<SOS>']]*BATCH_SIZE])).transpose(1,0)\n",
    "        # start_decode.shape = torch.Size([16, 1]) ==> start_decode = [2, 2, 2, 2, ..., 2] (word2index['<SOS>'] = 2)\n",
    "        slot_score, intent_score = decoder(start_decode,hidden_c,output,x_mask)\n",
    "\n",
    "        #print(slot_target.size()) ===> torch.Size([16, 50])\n",
    "        #print(slot_score.size()) ===> torch.Size([800, 122]) (800 = B*T = 16*50)\n",
    "        #print(intent_score.size()) ===> torch.Size([16, 22])\n",
    "\n",
    "        #print(intent_score)\n",
    "        _,intent_predicted = torch.max(intent_score,1)\n",
    "        \n",
    "        intent_truly_labeled += sum(intent_target == intent_predicted).item()\n",
    "        intent_mislabeled += sum(intent_target != intent_predicted).item()\n",
    "        \n",
    "        _,slot_predicted = torch.max(slot_score,1)\n",
    "        \n",
    "        #print(slot_target.size())\n",
    "        \n",
    "        slot_truly_labeled += sum(slot_target.view(-1) == slot_predicted).item()\n",
    "        slot_mislabeled += sum(slot_target.view(-1) != slot_predicted).item()\n",
    "        \n",
    "        loss_1 = loss_function_1(slot_score,slot_target.view(-1))\n",
    "        loss_2 = loss_function_2(intent_score,intent_target)\n",
    "\n",
    "        loss = loss_1+loss_2\n",
    "        losses.append(loss.data.numpy())\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm(encoder.parameters(), 5.0)\n",
    "        torch.nn.utils.clip_grad_norm(decoder.parameters(), 5.0)\n",
    "\n",
    "        enc_optim.step()\n",
    "        dec_optim.step()\n",
    "\n",
    "        if i % 100==0:\n",
    "            print(\"EPOCH\",epoch,\" batch\",i,\" : \",np.mean(losses))\n",
    "            \n",
    "            losses=[]\n",
    "                 \n",
    "    intent_acc.append(intent_truly_labeled / (intent_truly_labeled + intent_mislabeled))\n",
    "    slot_acc.append(slot_truly_labeled / (slot_truly_labeled + slot_mislabeled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.7497759856630825,\n",
       "  0.8407258064516129,\n",
       "  0.8830645161290323,\n",
       "  0.9245071684587813,\n",
       "  0.933915770609319],\n",
       " [0.8862044504181601,\n",
       "  0.9268518518518518,\n",
       "  0.9355249402628435,\n",
       "  0.9564590800477897,\n",
       "  0.9655988649940263])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_acc, slot_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_intent_acc = intent_acc\n",
    "train_slot_acc = slot_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embedding.weight',\n",
       "              tensor([[ 0.1684, -0.1990,  0.0759,  ..., -0.1788,  0.1600, -0.1141],\n",
       "                      [ 0.0027,  0.0046,  0.0570,  ...,  0.0389,  0.0709, -0.0529],\n",
       "                      [ 0.0329, -0.0408,  0.0381,  ..., -0.0825, -0.0179, -0.0938],\n",
       "                      ...,\n",
       "                      [-0.2382,  0.0178,  0.0556,  ..., -0.1748,  0.1299, -0.1411],\n",
       "                      [ 0.0330, -0.0316, -0.2120,  ...,  0.2453, -0.0847, -0.1772],\n",
       "                      [ 0.1296,  0.0298,  0.0086,  ...,  0.0663,  0.0854,  0.0099]])),\n",
       "             ('lstm.weight_ih_l0',\n",
       "              tensor([[ 0.4226, -0.0424, -0.0424,  ...,  0.2342, -0.1357, -0.0893],\n",
       "                      [ 0.4011, -0.5025,  0.2245,  ...,  0.0035,  0.0680,  0.2268],\n",
       "                      [ 0.6454, -0.1992,  0.1112,  ...,  0.5602, -0.1285, -0.1520],\n",
       "                      ...,\n",
       "                      [ 0.7072, -0.2868,  0.2543,  ...,  0.6171, -0.3054, -0.3099],\n",
       "                      [ 0.5540, -0.4018,  0.3686,  ...,  0.2227, -0.1731, -0.3801],\n",
       "                      [ 0.6534, -0.2252,  0.3509,  ...,  0.3722,  0.1469, -0.2211]])),\n",
       "             ('lstm.weight_hh_l0',\n",
       "              tensor([[ 2.2575e-01,  1.3474e-01,  1.5880e-01,  ..., -2.7249e-01,\n",
       "                        8.2784e-02, -1.4749e-01],\n",
       "                      [ 9.8434e-02,  2.2278e-03,  9.8434e-02,  ..., -1.8808e-01,\n",
       "                        8.4652e-02,  2.0986e-02],\n",
       "                      [-1.1560e-01,  1.2446e-01,  2.3244e-01,  ..., -1.2811e-01,\n",
       "                       -9.8454e-02,  6.3177e-02],\n",
       "                      ...,\n",
       "                      [-1.6085e-04,  2.6345e-01,  1.0086e-01,  ..., -1.9274e-01,\n",
       "                       -7.7750e-03, -8.1993e-02],\n",
       "                      [-4.3103e-02,  2.8017e-01,  2.2839e-01,  ..., -3.1387e-01,\n",
       "                        1.4910e-01,  2.1408e-01],\n",
       "                      [-2.2521e-02,  1.0628e-01,  3.8350e-02,  ..., -5.0207e-02,\n",
       "                       -1.1158e-01,  7.6715e-02]])),\n",
       "             ('lstm.bias_ih_l0',\n",
       "              tensor([ 3.7864e-01,  2.6804e-01,  9.7188e-02,  6.4741e-02,  1.4022e-01,\n",
       "                       2.8789e-01,  1.1649e-01,  9.4842e-02,  1.8246e-01, -9.2548e-03,\n",
       "                       3.6890e-02,  2.0276e-02,  1.9446e-01,  2.3783e-01,  5.4109e-02,\n",
       "                       2.8214e-01,  2.2761e-01,  1.4817e-01,  3.1097e-01,  1.6387e-01,\n",
       "                       7.2814e-02,  1.5508e-01,  2.1493e-01,  2.2111e-01,  3.2159e-01,\n",
       "                       2.1925e-01,  1.9513e-01,  9.2266e-02,  2.5489e-01,  1.6795e-01,\n",
       "                       1.2875e-01,  1.1490e-01,  1.3872e-01,  8.2529e-02, -9.5017e-03,\n",
       "                       2.2070e-01,  6.2542e-02,  9.0279e-02,  1.1368e-01,  1.0924e-01,\n",
       "                       2.3775e-03,  1.3755e-01,  9.8634e-02,  2.4176e-01,  1.8993e-01,\n",
       "                       2.3371e-01,  2.4918e-01, -1.3777e-02,  1.5665e-01, -9.8324e-03,\n",
       "                       1.2275e-01,  1.8610e-01,  1.4770e-01,  5.0702e-01,  1.9454e-01,\n",
       "                       1.4346e-01,  1.8895e-01,  5.0487e-02,  6.1139e-02,  1.1681e-01,\n",
       "                       6.1519e-02,  1.8555e-01,  1.8576e-01,  1.8339e-01,  1.9932e-01,\n",
       "                       6.4918e-02,  1.7751e-01, -3.9031e-02,  2.6935e-03,  8.2153e-02,\n",
       "                       6.8228e-02, -8.7761e-03,  2.0804e-01, -2.5749e-02,  9.8858e-02,\n",
       "                       4.7429e-02,  1.6084e-01,  1.4019e-01,  1.3298e-01,  2.7789e-01,\n",
       "                       1.2920e-01,  1.1750e-01,  2.5693e-01,  8.8421e-02, -3.8762e-02,\n",
       "                       1.1793e-01,  5.1532e-02,  9.5263e-02,  5.9906e-02,  4.4047e-02,\n",
       "                       1.5157e-01,  3.3107e-03, -2.1589e-02,  1.6716e-01,  2.0506e-01,\n",
       "                       1.5800e-01,  6.9752e-02,  1.5832e-01,  2.7236e-02,  2.4567e-01,\n",
       "                       6.4610e-03,  5.6906e-02,  5.8109e-02,  3.9973e-03,  7.3086e-02,\n",
       "                       3.0740e-01,  1.5537e-01,  2.3542e-01, -3.3789e-02,  2.6866e-02,\n",
       "                       8.1048e-02,  1.1271e-01,  8.8649e-02,  9.3383e-02,  1.2447e-02,\n",
       "                      -1.6011e-02,  8.9373e-02,  1.9438e-01,  1.6244e-01,  8.6203e-02,\n",
       "                      -7.0294e-04, -9.2710e-02,  1.1335e-01,  5.5232e-02, -1.0582e-01,\n",
       "                       8.8066e-02,  1.9262e-01,  2.1000e-01,  3.9778e-02, -1.2470e-01,\n",
       "                       1.1047e-01,  1.1652e-01, -7.3228e-02,  1.2724e-01, -5.8429e-02,\n",
       "                       6.2020e-02, -3.1599e-02,  8.9713e-02, -7.0752e-02, -5.6241e-02,\n",
       "                      -2.1325e-03, -1.9188e-01, -4.4312e-02, -2.1350e-02, -7.1115e-02,\n",
       "                       7.4531e-02,  6.2201e-02,  5.4542e-03,  1.1974e-02, -1.1459e-01,\n",
       "                      -8.5994e-02, -4.3499e-02, -8.5272e-02,  9.5380e-02, -1.0570e-02,\n",
       "                      -3.6826e-02,  1.4062e-01, -5.0805e-02,  2.4723e-02, -6.9935e-02,\n",
       "                       1.5026e-01, -5.7383e-02,  7.9475e-03, -1.0124e-01,  8.4443e-02,\n",
       "                      -9.4907e-02, -4.1857e-02, -1.3761e-02,  2.8866e-02, -4.8754e-02,\n",
       "                       4.6477e-02, -1.3235e-02, -9.9784e-02, -3.8958e-02,  1.1323e-02,\n",
       "                      -6.2167e-02,  6.8524e-02,  5.2873e-02, -6.8596e-02,  8.4248e-02,\n",
       "                      -9.1567e-02, -1.3846e-01, -2.3879e-02,  1.7208e-02, -7.1840e-02,\n",
       "                       2.4968e-02,  4.6314e-02,  1.1255e-01,  9.4069e-03, -1.4803e-01,\n",
       "                       8.2700e-02, -1.3118e-01,  4.2390e-01,  1.7231e-01,  1.9989e-01,\n",
       "                       3.8385e-02,  3.2218e-01,  1.0747e-01,  2.8237e-01,  1.1622e-01,\n",
       "                       2.0223e-01,  6.3848e-02,  1.4486e-01, -4.5732e-04,  2.5218e-01,\n",
       "                       3.4264e-01,  5.8964e-02,  3.9075e-01,  2.4606e-01,  4.5496e-02,\n",
       "                       3.9708e-01,  2.8749e-01,  7.0054e-02,  2.5046e-01,  9.8632e-02,\n",
       "                       2.4935e-01,  1.1736e-01,  1.0553e-01,  1.9473e-01, -1.2816e-02,\n",
       "                       1.1616e-01,  4.0752e-01,  2.9384e-01,  9.5155e-02,  4.5662e-02,\n",
       "                       3.4863e-02,  1.7511e-01,  5.4720e-01,  2.5591e-01,  1.4270e-01,\n",
       "                       6.1348e-02,  1.0459e-01,  3.2793e-01,  3.7025e-01,  2.7790e-01,\n",
       "                       1.3676e-01,  9.6302e-02,  1.9051e-01,  1.8006e-01,  1.7320e-01,\n",
       "                       6.8466e-02,  2.2080e-01,  2.1828e-01,  1.6610e-01,  2.6873e-01,\n",
       "                       4.5836e-01,  3.7889e-01,  2.6582e-01,  2.7853e-01,  2.0113e-01,\n",
       "                       5.8218e-02, -9.8935e-04,  1.5585e-01,  1.5194e-01,  3.4866e-01,\n",
       "                       1.9973e-01])),\n",
       "             ('lstm.bias_hh_l0',\n",
       "              tensor([ 0.3711,  0.3265,  0.1612, -0.0298,  0.2058,  0.0909,  0.1238,  0.1150,\n",
       "                       0.1842, -0.0278,  0.0533,  0.2170,  0.1001,  0.2754,  0.0785,  0.2859,\n",
       "                       0.1218,  0.0856,  0.1841,  0.0127,  0.1030,  0.1605, -0.0080,  0.1474,\n",
       "                       0.2219,  0.0794,  0.0312,  0.0686,  0.2689,  0.2685,  0.1783,  0.0572,\n",
       "                       0.0953,  0.0218, -0.0250,  0.3066,  0.0585,  0.1406,  0.1455,  0.1791,\n",
       "                      -0.0183,  0.3484,  0.1639,  0.0646,  0.0254,  0.2383,  0.1196, -0.0450,\n",
       "                       0.1544,  0.0073,  0.1532,  0.2440,  0.2975,  0.4423,  0.2724,  0.0089,\n",
       "                       0.1500, -0.0042,  0.0760,  0.1097,  0.0965,  0.3018,  0.0930,  0.2277,\n",
       "                       0.1052,  0.2561,  0.1693, -0.0296,  0.0970,  0.0682,  0.0058,  0.0852,\n",
       "                       0.1749,  0.0391,  0.1216,  0.1117,  0.0518,  0.1253,  0.0780,  0.2203,\n",
       "                       0.0325, -0.0103,  0.3168,  0.1036,  0.0955,  0.2601,  0.0857,  0.1571,\n",
       "                       0.0603,  0.2180,  0.2063,  0.1261, -0.0594,  0.0931, -0.0332,  0.1489,\n",
       "                       0.1101,  0.1305,  0.0641,  0.2025,  0.0998,  0.0161,  0.1007,  0.1182,\n",
       "                       0.1852,  0.2099,  0.2523,  0.2890, -0.0034,  0.0344,  0.2496,  0.0049,\n",
       "                       0.0892,  0.1563,  0.0034,  0.2250,  0.2288,  0.2583,  0.1376,  0.1873,\n",
       "                       0.2159, -0.0254,  0.0174,  0.1200,  0.0058,  0.1961,  0.1597, -0.0281,\n",
       "                       0.0167,  0.0665,  0.1303,  0.1322,  0.0687, -0.0848, -0.1078, -0.1167,\n",
       "                       0.0709,  0.0575,  0.0593,  0.0166,  0.1016, -0.1503, -0.0633,  0.1339,\n",
       "                       0.0568, -0.0648,  0.0452,  0.0263, -0.0846,  0.0215, -0.0483,  0.0090,\n",
       "                       0.0362,  0.0470,  0.0693,  0.0422,  0.0613,  0.0322, -0.0246, -0.1029,\n",
       "                       0.1217, -0.1008, -0.0406, -0.0484, -0.0626, -0.0104, -0.0977,  0.1084,\n",
       "                       0.0218, -0.0582, -0.0450, -0.0903, -0.0744,  0.1013,  0.0031,  0.0130,\n",
       "                      -0.0068, -0.0608, -0.0224, -0.0372, -0.0205, -0.0054, -0.0531,  0.1399,\n",
       "                       0.0397, -0.1161,  0.0964,  0.1266,  0.0809, -0.1307,  0.0088,  0.0808,\n",
       "                       0.4683,  0.3539,  0.2444,  0.0232,  0.2820,  0.2124,  0.3103,  0.2641,\n",
       "                       0.2778,  0.1457,  0.2521,  0.1530,  0.1929,  0.2946,  0.1745,  0.3179,\n",
       "                       0.3228,  0.1177,  0.2807,  0.0578,  0.2122,  0.1339,  0.1755,  0.2743,\n",
       "                       0.1836,  0.2418,  0.1464,  0.0640,  0.2018,  0.3501,  0.0752,  0.2248,\n",
       "                       0.2295,  0.1616,  0.0013,  0.3549,  0.0358,  0.2547,  0.2162,  0.2177,\n",
       "                       0.3139,  0.4071,  0.3038,  0.2729,  0.1807,  0.1608,  0.3159,  0.1156,\n",
       "                       0.2698,  0.1544,  0.1372,  0.2028,  0.3029,  0.5414,  0.4099,  0.2458,\n",
       "                       0.0803,  0.1741,  0.0843,  0.0775,  0.0600,  0.3698,  0.1686,  0.1763])),\n",
       "             ('lstm.weight_ih_l0_reverse',\n",
       "              tensor([[ 0.1264, -0.1433,  0.0930,  ..., -0.1361,  0.1747, -0.0652],\n",
       "                      [-0.0512,  0.0023, -0.0546,  ..., -0.0307,  0.0568, -0.0432],\n",
       "                      [ 0.1699,  0.0222,  0.0849,  ..., -0.0940,  0.0552,  0.0604],\n",
       "                      ...,\n",
       "                      [-0.3622,  0.4533,  0.2388,  ...,  0.1448, -0.0736,  0.5742],\n",
       "                      [-0.0135,  0.0279, -0.2818,  ...,  0.1766, -0.1267, -0.0569],\n",
       "                      [-0.4516,  0.3391,  0.8146,  ...,  0.3229,  0.0451,  0.3089]])),\n",
       "             ('lstm.weight_hh_l0_reverse',\n",
       "              tensor([[-0.1448, -0.1510, -0.0688,  ..., -0.0148,  0.0465, -0.0520],\n",
       "                      [ 0.0791, -0.1093, -0.0174,  ...,  0.0236,  0.0123, -0.0459],\n",
       "                      [-0.0321,  0.0488,  0.0922,  ..., -0.1721, -0.0663,  0.1199],\n",
       "                      ...,\n",
       "                      [-0.1012, -0.0724, -0.0546,  ...,  0.0460, -0.0361,  0.1330],\n",
       "                      [ 0.0220, -0.1388,  0.0702,  ...,  0.0993, -0.0263, -0.0059],\n",
       "                      [ 0.0539,  0.0499,  0.0124,  ...,  0.0230, -0.1198,  0.0636]])),\n",
       "             ('lstm.bias_ih_l0_reverse',\n",
       "              tensor([ 0.0222,  0.0413, -0.0095,  0.1131,  0.1278,  0.0417, -0.0288,  0.0725,\n",
       "                      -0.0620,  0.0171, -0.0087, -0.0770,  0.0633, -0.0117,  0.3690, -0.0460,\n",
       "                       0.1362,  0.1753,  0.0060,  0.0646, -0.0666,  0.0907, -0.0829,  0.1448,\n",
       "                       0.1281,  0.2704,  0.1054,  0.1550, -0.0961,  0.1671, -0.0882,  0.2726,\n",
       "                       0.2507,  0.0884,  0.1404, -0.0581,  0.0590,  0.1018,  0.0700,  0.1618,\n",
       "                      -0.0520,  0.0078,  0.1512,  0.0436,  0.1154, -0.0197, -0.0452,  0.2731,\n",
       "                      -0.0762,  0.1685,  0.1415, -0.0045,  0.1072,  0.0339, -0.0564,  0.1040,\n",
       "                       0.0279, -0.0360, -0.0770,  0.1306,  0.0519,  0.0894, -0.0820,  0.1320,\n",
       "                      -0.0755,  0.1285,  0.0323, -0.0800,  0.0197,  0.1428,  0.0100,  0.0084,\n",
       "                      -0.0238,  0.0715,  0.0706,  0.1672,  0.0405,  0.0217,  0.1285, -0.0101,\n",
       "                       0.0640,  0.0655,  0.1053,  0.1084,  0.0445,  0.1350,  0.0588, -0.0013,\n",
       "                       0.1308, -0.0548,  0.1002,  0.1450,  0.0166,  0.1527, -0.0415, -0.1219,\n",
       "                       0.0299,  0.0844, -0.0080,  0.0788, -0.0192,  0.1438,  0.0464, -0.0284,\n",
       "                      -0.0673, -0.0178,  0.1083,  0.1135, -0.0293, -0.0207, -0.0076, -0.1328,\n",
       "                      -0.0989,  0.0057, -0.0868,  0.1262, -0.0668,  0.1528,  0.1417,  0.0571,\n",
       "                       0.1470,  0.0128,  0.0020,  0.0087, -0.0688, -0.0112,  0.0664,  0.1362,\n",
       "                      -0.0548,  0.0934, -0.0296,  0.0888,  0.0924, -0.0117,  0.0173, -0.0232,\n",
       "                       0.0209, -0.1328,  0.0362, -0.0492,  0.0261, -0.1316, -0.0573,  0.0933,\n",
       "                       0.0691, -0.0850, -0.0407,  0.1020,  0.0986,  0.0672, -0.0590, -0.1204,\n",
       "                      -0.0169,  0.0476,  0.0806,  0.0381, -0.0030,  0.1151,  0.1409, -0.0030,\n",
       "                       0.0592,  0.1287, -0.0020,  0.0364, -0.0786, -0.0742,  0.0216, -0.0809,\n",
       "                       0.0079,  0.0324, -0.1503,  0.0409,  0.1222, -0.0215,  0.1015, -0.0845,\n",
       "                       0.1400, -0.0972,  0.0522,  0.0586, -0.0946,  0.0777,  0.0041,  0.0384,\n",
       "                      -0.1406, -0.0385,  0.0693,  0.1433, -0.1393, -0.0101, -0.1174,  0.0969,\n",
       "                      -0.0370,  0.0810, -0.0403,  0.1230, -0.0606, -0.0757,  0.0015, -0.0464,\n",
       "                      -0.0705, -0.1095, -0.0176,  0.0639, -0.0361,  0.0174,  0.3251, -0.0306,\n",
       "                       0.0835,  0.1898, -0.0616,  0.0646,  0.0849, -0.0734, -0.0556, -0.0131,\n",
       "                       0.0403,  0.1860,  0.0958, -0.1208,  0.0141,  0.2103,  0.1274,  0.2573,\n",
       "                       0.3195, -0.0720, -0.0300,  0.0286, -0.0158, -0.0757,  0.1205,  0.0859,\n",
       "                      -0.0163,  0.0212,  0.0590,  0.0274,  0.2219, -0.1009, -0.0682,  0.2713,\n",
       "                      -0.0301,  0.1658, -0.0299,  0.0914,  0.1856, -0.0102,  0.0642,  0.1454,\n",
       "                       0.0813, -0.0746,  0.1124,  0.2034, -0.1080, -0.0369,  0.1489, -0.0810])),\n",
       "             ('lstm.bias_hh_l0_reverse',\n",
       "              tensor([ 1.4503e-01,  9.3503e-02,  1.6276e-01,  3.3010e-01, -2.5916e-02,\n",
       "                      -7.0551e-03, -5.1562e-02,  8.5961e-02,  1.5997e-01,  1.3988e-01,\n",
       "                       1.2195e-01, -5.2660e-02,  1.2752e-01,  4.0966e-02,  3.2088e-01,\n",
       "                      -6.2193e-02,  2.1720e-01,  2.1186e-01,  2.9616e-02,  7.5101e-02,\n",
       "                      -1.1496e-02,  8.7714e-02,  5.0709e-02,  4.9443e-02,  7.7999e-02,\n",
       "                       3.7133e-01, -6.1558e-02,  1.5007e-01, -2.0299e-02,  3.3235e-01,\n",
       "                       4.4685e-02,  2.4282e-01,  2.9226e-01, -1.9037e-02,  1.1885e-04,\n",
       "                       1.7213e-01,  8.8657e-02,  1.4894e-01,  2.9139e-01,  1.8984e-01,\n",
       "                       1.5306e-01, -1.7786e-02,  8.9285e-02,  1.3147e-02,  2.0450e-01,\n",
       "                       1.2232e-01,  1.6971e-02,  1.5635e-01,  1.3939e-01,  1.7244e-01,\n",
       "                      -3.3221e-03,  1.7296e-01,  2.0509e-01, -2.9501e-02, -3.9461e-02,\n",
       "                      -2.8769e-02,  7.0118e-02,  4.2357e-02,  2.4565e-02,  1.8886e-01,\n",
       "                       1.6155e-02,  4.7917e-02, -8.0991e-02,  1.4064e-02,  8.7983e-02,\n",
       "                       1.6027e-01,  9.8117e-02, -3.2745e-02,  4.2797e-02, -5.0632e-02,\n",
       "                       1.6476e-01,  5.7500e-02,  4.6879e-02,  3.4127e-02,  6.2835e-02,\n",
       "                      -3.8815e-02, -1.0212e-02,  7.1984e-02, -6.7830e-02,  1.1191e-01,\n",
       "                       9.5180e-02, -1.6946e-02, -3.1889e-03, -9.9204e-02, -1.6958e-02,\n",
       "                      -8.6683e-02,  1.7428e-01, -2.1356e-02, -1.0402e-01,  8.2856e-02,\n",
       "                       3.7592e-02,  7.6622e-02,  2.8547e-02,  8.2876e-02,  9.4465e-02,\n",
       "                       1.0779e-01, -5.6975e-02,  1.3674e-01,  1.1920e-01, -3.4177e-02,\n",
       "                      -9.3747e-02, -5.5303e-02,  1.6269e-01, -7.8135e-02,  1.0531e-01,\n",
       "                       1.1247e-01,  1.0597e-01, -6.8921e-02,  8.2653e-02,  7.0395e-02,\n",
       "                       9.6895e-02, -9.6611e-02,  4.0032e-02,  3.8910e-02,  8.2311e-02,\n",
       "                       1.4210e-01, -6.3577e-03,  1.5937e-01,  6.6238e-02, -5.0464e-02,\n",
       "                      -1.8034e-02,  1.5838e-01,  1.3164e-01,  7.7952e-02,  6.1237e-02,\n",
       "                       1.7863e-01,  8.4517e-02,  8.7683e-02, -1.2248e-01, -1.5550e-01,\n",
       "                       1.3869e-01, -3.5078e-02,  1.1105e-02,  1.2487e-01, -9.5506e-02,\n",
       "                      -1.3998e-01, -1.3447e-01, -3.6841e-02,  7.6083e-02,  8.5399e-02,\n",
       "                       1.1035e-01, -4.3606e-03,  1.5146e-01,  6.1016e-02, -9.0080e-02,\n",
       "                       7.4203e-02, -1.5381e-02,  2.5990e-03, -7.4510e-02,  6.4378e-02,\n",
       "                      -1.2297e-01, -1.1422e-02,  6.1993e-02, -1.0434e-01,  2.9301e-02,\n",
       "                       7.1030e-02,  4.4073e-02, -3.3797e-02,  2.6150e-02,  4.9359e-02,\n",
       "                       4.6619e-02,  2.0317e-02, -1.0535e-01,  8.8444e-02, -1.2445e-01,\n",
       "                       1.1255e-01, -8.4572e-02,  7.4136e-03,  6.6079e-02,  5.8299e-02,\n",
       "                      -1.3942e-02, -1.6353e-01, -3.3532e-02, -1.1275e-01,  2.3613e-02,\n",
       "                      -6.0701e-02,  1.0314e-01, -4.9307e-02,  1.3032e-01,  9.6648e-02,\n",
       "                       1.1853e-01, -3.3065e-02,  5.4646e-02, -1.2220e-01, -1.5059e-01,\n",
       "                      -8.7195e-02,  8.4678e-02, -5.4129e-02, -1.1441e-01, -4.5103e-02,\n",
       "                      -1.3932e-01, -1.8371e-02, -3.8908e-02, -5.5046e-02, -2.7101e-02,\n",
       "                       9.0378e-02, -1.0335e-01,  5.0626e-02,  1.0372e-01, -1.3587e-02,\n",
       "                       1.4146e-02,  3.2455e-02,  8.8192e-02, -9.0804e-03,  1.2571e-01,\n",
       "                       1.2348e-01,  1.8872e-01, -4.3822e-02,  1.7328e-01,  7.0846e-02,\n",
       "                       1.6623e-01,  1.1095e-01,  1.3710e-01, -3.2480e-02, -4.7362e-02,\n",
       "                       8.7546e-02,  1.9714e-01,  2.8551e-01, -4.4140e-02, -1.0995e-01,\n",
       "                      -5.8498e-02,  2.3198e-01,  7.8149e-02,  8.8168e-02,  2.7771e-01,\n",
       "                       7.4517e-02, -7.5165e-03,  3.5035e-02,  1.2220e-01, -6.6915e-02,\n",
       "                       1.9523e-01, -1.6543e-02, -4.4177e-02,  3.0372e-02, -5.2318e-03,\n",
       "                      -1.1111e-02,  1.3533e-01, -7.1206e-02,  2.9436e-03,  1.0576e-01,\n",
       "                       3.0171e-02,  3.5127e-01,  1.4465e-01,  2.3013e-02,  1.9621e-01,\n",
       "                       4.9517e-02,  1.4417e-01,  1.7374e-01,  7.7033e-02, -7.0552e-03,\n",
       "                       4.0574e-02,  2.3211e-01,  1.2483e-01,  7.1559e-02,  1.0039e-01,\n",
       "                       5.7219e-03]))])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embedding.weight',\n",
       "              tensor([[ 0.0691, -0.1286,  0.1308,  ..., -0.0307,  0.0991, -0.0579],\n",
       "                      [-0.0789, -0.0999, -0.0474,  ..., -0.0974, -0.0409, -0.0069],\n",
       "                      [ 0.3179, -0.1235, -0.0877,  ...,  0.5900,  0.1240,  0.3409],\n",
       "                      ...,\n",
       "                      [ 0.3077, -0.4131,  0.3278,  ..., -0.3472,  0.3740, -0.3759],\n",
       "                      [-0.0246,  0.0926, -0.0192,  ...,  0.0010, -0.0955, -0.0400],\n",
       "                      [ 0.2618, -0.3421,  0.2751,  ..., -0.6791,  0.0608, -0.5264]])),\n",
       "             ('lstm.weight_ih_l0',\n",
       "              tensor([[ 0.0087, -0.0722,  0.0966,  ...,  0.0225, -0.1046,  0.0277],\n",
       "                      [ 0.0574, -0.0525,  0.0890,  ..., -0.0577, -0.0264,  0.1666],\n",
       "                      [ 0.0287, -0.0366,  0.0901,  ..., -0.0104, -0.0711,  0.0359],\n",
       "                      ...,\n",
       "                      [ 0.1209, -0.1646,  0.2160,  ...,  0.3461,  0.0481, -0.4790],\n",
       "                      [ 0.0595, -0.0660, -0.0052,  ...,  0.0233, -0.0250, -0.0026],\n",
       "                      [-0.0321, -0.0380,  0.0360,  ..., -0.0629, -0.0244,  0.3845]])),\n",
       "             ('lstm.weight_hh_l0',\n",
       "              tensor([[ 0.0917, -0.0012,  0.0947,  ..., -0.0555,  0.1039,  0.0811],\n",
       "                      [ 0.0561, -0.0083, -0.0201,  ..., -0.0225, -0.0588, -0.0440],\n",
       "                      [ 0.0783, -0.0819,  0.0755,  ...,  0.0365, -0.0050, -0.0114],\n",
       "                      ...,\n",
       "                      [ 0.0591, -0.1491,  0.0516,  ..., -0.0101,  0.1036,  0.2021],\n",
       "                      [ 0.0958, -0.1195,  0.0781,  ..., -0.0924,  0.0789,  0.1128],\n",
       "                      [-0.0192,  0.0696, -0.0710,  ...,  0.0419,  0.0698,  0.0672]])),\n",
       "             ('lstm.bias_ih_l0',\n",
       "              tensor([-0.0062,  0.0888,  0.0091,  0.0772, -0.0563,  0.0564,  0.0592,  0.0404,\n",
       "                      -0.0397, -0.0080,  0.0597,  0.0891,  0.0527,  0.0429,  0.0304,  0.0362,\n",
       "                      -0.0537, -0.0079, -0.0234,  0.0479, -0.0148, -0.0102,  0.0442,  0.0972,\n",
       "                       0.0467, -0.0583, -0.0018,  0.0547,  0.0674,  0.0049,  0.0493,  0.0956,\n",
       "                       0.0593,  0.0380,  0.0496,  0.0289,  0.0193,  0.0142,  0.0947, -0.0322,\n",
       "                       0.0323,  0.0264, -0.0582, -0.0312,  0.1243, -0.0259,  0.0831,  0.1177,\n",
       "                       0.0168,  0.1221, -0.0037,  0.0315,  0.0411, -0.0061, -0.0404,  0.0414,\n",
       "                       0.0672, -0.0248,  0.0104,  0.0392,  0.0423,  0.0585,  0.0927, -0.0118,\n",
       "                      -0.0200, -0.0308,  0.0679,  0.1101, -0.0174, -0.0232, -0.0046, -0.0127,\n",
       "                       0.0814,  0.0951, -0.0335,  0.0619,  0.1359,  0.0239,  0.0114,  0.0520,\n",
       "                       0.0299,  0.0498,  0.0288,  0.1879,  0.0628,  0.0722,  0.0869,  0.1036,\n",
       "                       0.0395,  0.0160, -0.0543,  0.1080,  0.0219, -0.0082, -0.0574, -0.0288,\n",
       "                      -0.0115,  0.0607, -0.0180, -0.0537, -0.0214,  0.0025, -0.0079,  0.0902,\n",
       "                       0.0652,  0.0692,  0.0120,  0.0548, -0.0227,  0.0243,  0.1062, -0.0272,\n",
       "                      -0.0568,  0.0149,  0.1411, -0.0199, -0.0523, -0.0577,  0.1082, -0.0403,\n",
       "                       0.1119,  0.0803,  0.0153,  0.0557,  0.0264,  0.0634,  0.1132,  0.0329,\n",
       "                       0.0915,  0.0743,  0.0060,  0.0079,  0.0122,  0.0340,  0.0878, -0.0203,\n",
       "                       0.0116, -0.0323,  0.0771,  0.0231,  0.0490,  0.0729,  0.0702, -0.0217,\n",
       "                       0.0246, -0.0294,  0.0932, -0.0421, -0.0487,  0.0551,  0.0498,  0.0048,\n",
       "                      -0.0239,  0.0529,  0.0635, -0.0057,  0.0563, -0.0156,  0.1033, -0.0301,\n",
       "                       0.1078,  0.0287,  0.0489,  0.0562,  0.0400,  0.0003, -0.0820,  0.0640,\n",
       "                      -0.0142, -0.0326,  0.0107,  0.0583,  0.0003,  0.0923,  0.0444, -0.0296,\n",
       "                       0.0123, -0.0113,  0.0857,  0.0766,  0.1128,  0.1105,  0.0294,  0.0342,\n",
       "                       0.0610, -0.0357,  0.1006, -0.0454,  0.0547, -0.0406,  0.1030, -0.0143,\n",
       "                       0.0900,  0.1014,  0.0915, -0.0469, -0.0655,  0.0953, -0.0830,  0.0420,\n",
       "                      -0.0046, -0.0611,  0.0664,  0.0243,  0.0641, -0.0088,  0.0178, -0.0307,\n",
       "                       0.0234,  0.1023, -0.0267,  0.1058, -0.0126, -0.0200,  0.0310, -0.0745,\n",
       "                      -0.0033,  0.0386, -0.0012,  0.0188, -0.0526,  0.0981, -0.0091, -0.0509,\n",
       "                       0.0781, -0.0321,  0.0276,  0.0982, -0.0733,  0.0332,  0.0333, -0.0024,\n",
       "                      -0.0493,  0.0440,  0.0288, -0.0761,  0.0701,  0.1006,  0.1016,  0.0633,\n",
       "                      -0.0460,  0.0269, -0.0831, -0.0163,  0.0752,  0.0217, -0.0587, -0.0242,\n",
       "                      -0.0318, -0.0020, -0.0397,  0.0398,  0.0043,  0.0954, -0.0202,  0.0945,\n",
       "                       0.0418,  0.0136,  0.1038, -0.0255, -0.0017,  0.1080,  0.0427, -0.0032,\n",
       "                       0.1092, -0.0762, -0.0525, -0.0196,  0.0707, -0.0172, -0.0877,  0.0377,\n",
       "                      -0.0065, -0.0022, -0.0189,  0.0478, -0.0292,  0.0112,  0.0897, -0.0147,\n",
       "                      -0.0371, -0.0521,  0.0798,  0.0736, -0.1064, -0.0009,  0.0771,  0.0260,\n",
       "                      -0.0142, -0.0838,  0.0322,  0.0502, -0.0401,  0.0926,  0.0203, -0.0691,\n",
       "                       0.0482, -0.0162, -0.1111, -0.0043,  0.0427,  0.0766,  0.0235,  0.0229,\n",
       "                      -0.0062,  0.0542, -0.0190, -0.0833,  0.0731, -0.0436, -0.0602,  0.0918,\n",
       "                       0.0456, -0.0942,  0.0343, -0.0481, -0.0925, -0.0197,  0.0229,  0.1117,\n",
       "                       0.0060,  0.0995,  0.0233,  0.0213, -0.0442, -0.0113,  0.0176, -0.0726,\n",
       "                      -0.0350,  0.0530,  0.0273,  0.0201, -0.0041,  0.0776, -0.0447,  0.0050,\n",
       "                      -0.0929, -0.0603, -0.0083,  0.0721,  0.1133,  0.1098, -0.0370, -0.0800,\n",
       "                       0.0121, -0.0387,  0.0737,  0.0446,  0.0239, -0.0562, -0.0934, -0.0845,\n",
       "                       0.0636, -0.0319,  0.0672, -0.0092,  0.0500, -0.0502,  0.0350,  0.0559,\n",
       "                       0.0854,  0.0619, -0.0763,  0.0358, -0.0758,  0.0887,  0.0627, -0.0801,\n",
       "                       0.0383, -0.0121,  0.0033,  0.0295, -0.0936, -0.0020, -0.0169,  0.0081,\n",
       "                      -0.0552,  0.0447,  0.0744, -0.0983, -0.0549, -0.0198, -0.0216,  0.0675,\n",
       "                      -0.0248,  0.0352,  0.0906,  0.0294,  0.0957,  0.0311,  0.0864, -0.0468,\n",
       "                       0.0593,  0.1103,  0.0592, -0.0153, -0.0404, -0.0563, -0.0251,  0.0521,\n",
       "                      -0.0353, -0.0164,  0.1388, -0.0759,  0.0149,  0.0887, -0.0049, -0.0131,\n",
       "                      -0.0117, -0.0048,  0.0720,  0.0193,  0.0038,  0.0310,  0.0499,  0.1356,\n",
       "                      -0.0669,  0.0236,  0.0120,  0.0193,  0.0524,  0.1313,  0.0743,  0.0170,\n",
       "                       0.0461,  0.0043, -0.0839,  0.0254,  0.0684,  0.0728,  0.1004,  0.1207,\n",
       "                      -0.0159,  0.0632, -0.0373, -0.0446,  0.1182,  0.0447,  0.0815,  0.0799,\n",
       "                      -0.0768,  0.0750,  0.0775, -0.0459,  0.0755, -0.0128, -0.0437, -0.0508,\n",
       "                       0.0002,  0.0254, -0.0666,  0.0672, -0.0342, -0.0454, -0.0097,  0.0812,\n",
       "                       0.0141,  0.0120,  0.0784,  0.0035,  0.0883, -0.0257,  0.0473, -0.1278,\n",
       "                       0.0034,  0.0485, -0.0194,  0.1557, -0.0200,  0.0714,  0.0213, -0.1256,\n",
       "                      -0.0646,  0.0915,  0.1019,  0.0501,  0.1085,  0.1003,  0.0166, -0.0338,\n",
       "                       0.0992,  0.0389,  0.0480,  0.0424, -0.0560,  0.0965,  0.0992, -0.0423,\n",
       "                      -0.0472,  0.0246,  0.0482,  0.0905, -0.0480,  0.0699,  0.0489,  0.0061,\n",
       "                      -0.0709,  0.0296,  0.0695, -0.0038,  0.0704, -0.0713,  0.0857, -0.0730,\n",
       "                       0.0652,  0.0576,  0.0044,  0.0142, -0.0219, -0.0105,  0.0543, -0.0214])),\n",
       "             ('lstm.bias_hh_l0',\n",
       "              tensor([ 6.7375e-02, -6.0628e-02,  8.7502e-02,  9.3736e-02,  3.1443e-02,\n",
       "                       2.6047e-02,  1.0370e-01, -5.4829e-03,  4.9925e-02,  6.1939e-02,\n",
       "                       8.1827e-02,  3.7215e-02,  3.9510e-02,  1.0800e-01,  7.6906e-02,\n",
       "                      -6.6409e-03,  6.3096e-02,  4.3977e-03, -4.6048e-02,  6.4845e-02,\n",
       "                      -5.0045e-02,  1.7447e-02, -4.1833e-02, -4.2072e-02, -3.6019e-02,\n",
       "                       5.8870e-02,  9.5454e-02,  6.7904e-02, -6.1910e-03,  5.4690e-02,\n",
       "                       8.4480e-03,  1.3426e-01, -3.7640e-02, -4.7434e-02,  7.0245e-02,\n",
       "                      -1.6138e-02,  6.8524e-02, -2.0404e-02,  1.0810e-01, -4.5373e-02,\n",
       "                       1.0983e-01, -5.6865e-02, -2.1650e-02,  8.8873e-02,  1.2025e-01,\n",
       "                       4.3907e-02,  1.2197e-01,  1.5511e-01,  9.2700e-02, -2.9263e-02,\n",
       "                       1.0983e-01,  4.5897e-02,  2.8287e-02,  9.0935e-02,  4.3983e-02,\n",
       "                      -4.5043e-02, -5.1166e-02, -2.2749e-04, -4.7912e-02,  6.5427e-02,\n",
       "                       9.5099e-02,  7.7893e-03,  7.9509e-02, -5.8412e-02,  9.5753e-02,\n",
       "                      -6.3107e-02,  1.1693e-01, -5.3544e-03,  8.6336e-02, -3.6996e-02,\n",
       "                       1.2546e-01,  8.9040e-02, -1.1881e-02,  7.1866e-02, -5.0029e-02,\n",
       "                       8.4030e-02,  2.3687e-02,  6.1852e-02, -3.3093e-02,  2.4347e-02,\n",
       "                       5.8900e-02,  4.9026e-02, -3.4523e-02,  1.6594e-01,  9.6252e-02,\n",
       "                      -4.4567e-02,  6.8595e-02,  5.5338e-02, -4.0323e-02,  8.3380e-02,\n",
       "                       4.2234e-03,  9.6755e-02, -1.1073e-02,  3.4663e-02, -6.2432e-02,\n",
       "                       8.3152e-02,  5.5132e-02, -1.4239e-02,  3.3000e-03, -1.2549e-02,\n",
       "                       9.4069e-02,  1.1079e-01,  8.6632e-03,  7.6512e-02,  6.8944e-02,\n",
       "                      -3.4775e-02,  7.0072e-02,  3.4838e-02,  6.2021e-02,  5.2840e-02,\n",
       "                      -2.0126e-02,  1.0797e-01,  5.1328e-02,  7.8883e-02,  6.7414e-03,\n",
       "                       8.8573e-02,  5.0987e-02,  4.2586e-02,  7.8992e-03,  2.6106e-04,\n",
       "                       3.2938e-02,  9.5844e-02,  1.9741e-02, -2.9998e-02,  4.1093e-02,\n",
       "                       3.3035e-02, -3.2380e-02, -2.4055e-02, -2.5706e-02,  2.9961e-02,\n",
       "                       9.6877e-02, -1.5423e-02,  7.6233e-02,  1.9513e-03,  6.8110e-02,\n",
       "                      -5.4429e-02, -4.1603e-02,  7.0540e-02, -4.4845e-02, -2.8104e-02,\n",
       "                      -2.4922e-02,  5.1748e-02, -4.4966e-02,  6.4572e-02,  7.5952e-02,\n",
       "                       3.1233e-02,  5.1464e-02,  7.5349e-02,  4.0878e-03,  6.3932e-02,\n",
       "                       8.5822e-02, -3.9782e-02,  4.0157e-02,  9.4247e-02,  2.3956e-02,\n",
       "                       9.7014e-03, -4.6732e-02,  7.0457e-02,  8.2366e-02,  8.9731e-02,\n",
       "                       6.7169e-02,  2.6570e-02,  4.0823e-02,  1.0105e-01,  4.0999e-02,\n",
       "                       1.0599e-01, -4.0883e-02, -6.3569e-02, -3.8788e-02,  3.2419e-02,\n",
       "                      -1.8191e-02, -6.9815e-03,  3.9805e-02, -5.3520e-02, -3.2665e-02,\n",
       "                      -2.2734e-02, -2.5435e-02,  1.0570e-01,  1.4667e-02,  1.5732e-02,\n",
       "                       7.5908e-02,  8.7780e-02, -8.7748e-02,  1.0336e-01,  1.0965e-01,\n",
       "                       1.3013e-02,  8.3907e-02, -5.9986e-02, -2.6238e-02, -5.0280e-02,\n",
       "                       1.8811e-04,  8.2220e-02,  1.0411e-01, -2.5209e-02, -5.0731e-02,\n",
       "                       2.5552e-02, -8.1791e-02, -5.4458e-02,  3.1733e-02, -1.5067e-02,\n",
       "                      -4.2597e-03, -5.5121e-02, -1.4793e-04, -2.2729e-02, -6.4982e-02,\n",
       "                       9.7857e-03, -3.0389e-02,  7.3702e-02, -2.6831e-02, -6.9968e-03,\n",
       "                      -7.1262e-03,  7.1878e-02,  2.1066e-03,  8.6436e-02,  1.0877e-02,\n",
       "                       4.1897e-02, -6.0925e-02, -4.9047e-02,  1.1671e-02,  4.5965e-02,\n",
       "                       1.5466e-02,  9.8103e-02, -1.3611e-02,  9.9707e-02, -2.4101e-02,\n",
       "                       5.5578e-03,  3.2675e-02, -1.5580e-02,  2.4336e-02,  7.2661e-02,\n",
       "                       1.0560e-01,  1.1099e-02, -4.3873e-02,  1.0685e-02,  4.3380e-02,\n",
       "                      -2.0260e-02,  8.7606e-03, -3.5788e-02,  4.3908e-02,  6.9335e-02,\n",
       "                       1.0090e-01,  7.1052e-02, -8.1460e-02, -3.3838e-02,  6.2742e-02,\n",
       "                       2.9153e-02,  9.0361e-02, -3.2169e-02, -5.5794e-02, -2.6179e-02,\n",
       "                      -6.3112e-02,  5.6289e-02,  7.1000e-02, -1.8635e-02, -6.8965e-03,\n",
       "                      -2.0091e-02, -4.4336e-03, -1.1384e-01,  3.3215e-02, -5.6995e-02,\n",
       "                       3.2470e-02,  1.5508e-02,  8.2406e-02,  9.9058e-02,  9.0074e-02,\n",
       "                      -8.4986e-02,  2.0138e-03,  8.9102e-03,  1.1336e-01,  8.9407e-02,\n",
       "                      -8.4802e-02, -2.1940e-02, -9.2540e-02,  8.1093e-02, -9.3686e-03,\n",
       "                       1.0199e-01, -5.5159e-02, -1.1919e-02,  1.0889e-01,  4.5121e-02,\n",
       "                      -1.0807e-01,  6.2496e-02, -3.1164e-02,  5.9282e-02, -4.4988e-02,\n",
       "                       6.2870e-02,  6.0825e-02,  9.2163e-02, -9.9985e-02, -3.3780e-02,\n",
       "                      -3.4934e-02, -1.5072e-02, -1.8116e-03,  5.0992e-02,  4.5897e-02,\n",
       "                      -5.0574e-03,  4.3463e-02, -5.3009e-02, -8.9743e-02,  3.4928e-02,\n",
       "                      -2.9386e-02,  2.9218e-02,  6.6720e-02,  6.4105e-02,  7.4515e-02,\n",
       "                      -6.3380e-02,  3.7469e-02,  2.1374e-02,  9.0284e-02,  1.0561e-01,\n",
       "                       2.2740e-02,  9.7578e-02,  3.1535e-02,  1.9968e-02, -8.5756e-02,\n",
       "                       1.0886e-01, -8.3635e-02, -6.7937e-02, -3.2800e-02,  1.0516e-01,\n",
       "                      -3.7525e-02, -1.5570e-03,  2.4860e-02, -6.6527e-02,  9.5752e-03,\n",
       "                      -2.8589e-02,  3.1715e-02,  6.2953e-03, -8.4395e-03,  5.4305e-02,\n",
       "                      -5.2772e-02, -9.7442e-03,  3.5638e-03,  4.0642e-02, -1.5015e-02,\n",
       "                      -4.2813e-03, -7.7740e-02, -1.0964e-01, -5.9355e-03,  9.5396e-04,\n",
       "                      -4.4064e-03,  9.0410e-02,  1.5563e-02, -8.0245e-02, -6.3029e-02,\n",
       "                      -7.1832e-03,  2.3479e-02, -1.5962e-02, -2.8091e-02,  5.7309e-02,\n",
       "                      -8.4055e-02, -5.6057e-02, -1.0583e-01,  5.2679e-02,  1.4315e-02,\n",
       "                       2.0195e-02,  2.9726e-02,  3.6882e-02, -1.7377e-02,  3.5309e-02,\n",
       "                      -1.6354e-02,  1.0153e-01, -6.9545e-02,  6.7534e-02, -1.9995e-02,\n",
       "                      -4.3330e-02,  1.0816e-01,  9.5681e-04, -2.3772e-02,  1.1342e-01,\n",
       "                       1.2892e-02, -7.6472e-03,  2.5519e-02, -1.0072e-01,  9.4870e-02,\n",
       "                       1.0210e-01, -3.2336e-02,  6.7986e-02, -2.5226e-02, -7.7351e-02,\n",
       "                       5.6170e-02, -3.9855e-02, -3.7899e-02,  9.9367e-02,  6.8568e-02,\n",
       "                       1.0743e-01,  5.4532e-02, -1.7975e-02,  6.4154e-02,  3.8490e-02,\n",
       "                      -1.3863e-02, -3.6615e-02, -7.2620e-02,  9.9073e-02,  1.4790e-01,\n",
       "                      -2.3748e-02, -6.8770e-02,  2.4858e-03,  1.8620e-03,  1.1195e-01,\n",
       "                       2.2901e-02,  1.7269e-03,  2.4037e-02, -6.7683e-02, -1.2945e-01,\n",
       "                       5.3454e-02, -2.9311e-02, -4.4230e-02,  6.5893e-02, -1.0487e-01,\n",
       "                       5.5676e-02,  3.5706e-02, -3.9560e-02, -1.2021e-02,  4.5228e-02,\n",
       "                       1.5021e-01,  4.8596e-02, -2.3234e-02,  1.5097e-03, -3.7606e-02,\n",
       "                       4.0459e-02,  1.2934e-01,  8.0002e-02,  1.4261e-02, -5.6485e-02,\n",
       "                      -9.8102e-02,  2.5860e-02,  4.8493e-02,  1.1797e-01, -4.5707e-02,\n",
       "                       5.4918e-02, -1.1619e-02,  2.1925e-03,  1.5006e-02,  5.6446e-03,\n",
       "                       7.1375e-03,  9.0989e-02,  8.7065e-02,  6.6777e-02,  3.4013e-02,\n",
       "                      -1.3232e-02, -8.0744e-02,  3.7339e-02,  8.4059e-02,  2.8175e-02,\n",
       "                       6.7659e-02,  7.0859e-03, -2.3162e-03,  1.1583e-01, -5.7497e-02,\n",
       "                       3.4626e-02,  3.6588e-02,  4.7142e-02,  1.4055e-02,  2.9344e-02,\n",
       "                       6.1371e-03, -5.2735e-02,  3.0621e-02,  6.7630e-02,  4.1379e-02,\n",
       "                       2.1019e-02,  4.2884e-02, -6.6598e-02, -3.2435e-02,  7.4930e-02,\n",
       "                       1.9728e-03,  1.7281e-02,  1.9597e-02,  3.5206e-02,  6.4896e-02,\n",
       "                       4.9078e-02, -8.4995e-02, -4.2955e-02,  1.0259e-01,  1.0383e-01,\n",
       "                       3.6248e-02, -4.7080e-02,  8.1545e-02,  2.4879e-02,  6.9297e-02,\n",
       "                       1.1157e-01,  4.2946e-02,  5.8295e-02,  3.1718e-02, -1.0599e-02,\n",
       "                       7.2571e-03, -1.2049e-02,  8.1660e-03,  1.3574e-02, -4.4103e-02,\n",
       "                      -1.8780e-02,  1.0717e-01,  4.6067e-02,  1.6078e-02, -4.1261e-02,\n",
       "                       8.5571e-02,  4.3557e-02,  1.7172e-02,  1.1082e-02,  4.1292e-02,\n",
       "                       2.4376e-02, -8.6666e-02, -3.6332e-02, -1.1496e-01, -3.8267e-02,\n",
       "                       7.9196e-02, -5.3152e-02,  3.0039e-02, -3.3557e-02,  3.1197e-02,\n",
       "                      -3.3644e-02,  6.6820e-02])),\n",
       "             ('attn.weight',\n",
       "              tensor([[-0.0884,  0.0486,  0.0602,  ...,  0.1030,  0.1411, -0.0920],\n",
       "                      [-0.0502, -0.1158,  0.0170,  ..., -0.1083, -0.0189,  0.1830],\n",
       "                      [-0.1403,  0.0792, -0.0666,  ...,  0.0430, -0.0110,  0.0374],\n",
       "                      ...,\n",
       "                      [ 0.0078,  0.0395, -0.1073,  ...,  0.0445, -0.0016, -0.0282],\n",
       "                      [-0.1077, -0.0487,  0.1383,  ...,  0.1820,  0.0395, -0.0247],\n",
       "                      [-0.0798,  0.1385,  0.0956,  ..., -0.0913,  0.1120, -0.0295]])),\n",
       "             ('attn.bias',\n",
       "              tensor([ 0.0303, -0.0048,  0.0380,  0.0369, -0.0162,  0.0237,  0.0670,  0.0293,\n",
       "                       0.0334, -0.0459, -0.0831, -0.0836, -0.0134, -0.0767,  0.0112,  0.0038,\n",
       "                      -0.0281, -0.0111, -0.0613, -0.0240,  0.0082, -0.0716, -0.0173,  0.0006,\n",
       "                      -0.0617, -0.0083, -0.0100, -0.0099, -0.0559, -0.0330, -0.0685,  0.0828,\n",
       "                      -0.0428,  0.0626, -0.0336,  0.0098,  0.0181,  0.0766,  0.0745, -0.0372,\n",
       "                       0.0209, -0.0572,  0.0068,  0.0322,  0.0780, -0.0436,  0.0012, -0.0326,\n",
       "                      -0.0349, -0.0865, -0.0796,  0.0738, -0.0481, -0.0809,  0.0250, -0.0465,\n",
       "                      -0.0606,  0.0576,  0.0651, -0.0798, -0.0173, -0.0694,  0.0120, -0.0205,\n",
       "                       0.0824,  0.0856,  0.0421, -0.0629,  0.0816,  0.0362,  0.0684, -0.0024,\n",
       "                       0.0806, -0.0261, -0.0829,  0.0488, -0.0010, -0.0031, -0.0202, -0.0555,\n",
       "                      -0.0300,  0.0551,  0.0331,  0.0342, -0.0619,  0.0671, -0.0542, -0.0548,\n",
       "                      -0.0168,  0.0128, -0.0397, -0.0653, -0.0631,  0.0570, -0.0498, -0.0705,\n",
       "                       0.0367, -0.0660, -0.0678, -0.0844,  0.0017,  0.0802, -0.0180, -0.0289,\n",
       "                      -0.0761, -0.0856,  0.0652, -0.0141, -0.0103,  0.0269,  0.0085, -0.0631,\n",
       "                       0.0840,  0.0560,  0.0479, -0.0383, -0.0647,  0.0725,  0.0314,  0.0361,\n",
       "                       0.0211,  0.0081,  0.0539,  0.0121, -0.0127,  0.0110,  0.0204, -0.0486])),\n",
       "             ('slot_out.weight',\n",
       "              tensor([[ 0.1736, -0.0749,  0.0532,  ...,  0.0880, -0.0213,  0.0013],\n",
       "                      [-0.0768,  0.0246, -0.0902,  ...,  0.1696,  0.1270, -0.1485],\n",
       "                      [-0.0931,  0.0469, -0.0718,  ...,  0.1277,  0.0489, -0.0089],\n",
       "                      ...,\n",
       "                      [-0.1802, -0.1007,  0.2063,  ..., -0.0770,  0.0308, -0.0961],\n",
       "                      [ 0.0648,  0.0145,  0.0156,  ..., -0.0557, -0.0247, -0.0028],\n",
       "                      [-0.2295,  0.0431,  0.2679,  ...,  0.0437, -0.0482, -0.1704]])),\n",
       "             ('slot_out.bias',\n",
       "              tensor([ 0.0583, -0.0837, -0.0457,  0.0392,  0.0288, -0.0519, -0.0468,  0.0162,\n",
       "                       0.0283,  0.0369,  0.0012, -0.0409,  0.0134, -0.0119,  0.0316, -0.0734,\n",
       "                       0.0129, -0.0525, -0.0420, -0.0175,  0.0449, -0.0406,  0.0141, -0.0111,\n",
       "                      -0.0474, -0.0252, -0.0708,  0.0354,  0.0394, -0.0288,  0.0179, -0.0240,\n",
       "                       0.0714, -0.0181, -0.0611, -0.0646,  0.0374, -0.0036, -0.0216,  0.0556,\n",
       "                      -0.0354, -0.0450, -0.0593,  0.0079, -0.0643,  0.0214,  0.0087,  0.0700,\n",
       "                      -0.0387, -0.0544, -0.0348, -0.0252, -0.0461,  0.0125, -0.0467, -0.0224,\n",
       "                      -0.0472, -0.0422, -0.0483,  0.0309, -0.0360, -0.0520,  0.0385,  0.0198,\n",
       "                      -0.0361, -0.0485, -0.0041,  0.0309, -0.1112, -0.0553, -0.0850, -0.0358,\n",
       "                      -0.0118, -0.0434, -0.0654, -0.0354, -0.0865, -0.0664, -0.0546,  0.0452,\n",
       "                      -0.0569,  0.0195, -0.0094, -0.0960, -0.0489, -0.0673, -0.0482, -0.0622,\n",
       "                      -0.0050, -0.0049, -0.0521,  0.0146,  0.0286,  0.0413, -0.0222,  0.0482,\n",
       "                      -0.0005, -0.0141, -0.0440, -0.0141,  0.0185,  0.0262, -0.0002, -0.0498,\n",
       "                      -0.0840, -0.0291, -0.0461,  0.0186, -0.0228,  0.0166,  0.0100, -0.0813,\n",
       "                       0.0203, -0.0083,  0.0353,  0.0058,  0.0147,  0.0500, -0.0738, -0.0646,\n",
       "                       0.0221,  0.0593])),\n",
       "             ('intent_out.weight',\n",
       "              tensor([[-0.0206,  0.0311, -0.0248,  ...,  0.2712,  0.2496, -0.1072],\n",
       "                      [ 0.0003,  0.0470, -0.0222,  ...,  0.0368, -0.0347, -0.0945],\n",
       "                      [-0.0122, -0.0372, -0.0489,  ...,  0.1128,  0.1197, -0.0506],\n",
       "                      ...,\n",
       "                      [-0.0551,  0.0491, -0.0474,  ...,  0.0303,  0.0936,  0.0234],\n",
       "                      [-0.0390,  0.0521, -0.0555,  ..., -0.0192,  0.0490,  0.0463],\n",
       "                      [ 0.0235, -0.0200,  0.0577,  ...,  0.1031,  0.1504, -0.1151]])),\n",
       "             ('intent_out.bias',\n",
       "              tensor([-0.0316, -0.0701, -0.0211,  0.0218, -0.0749,  0.0459,  0.0634, -0.0515,\n",
       "                      -0.1219, -0.0069,  0.0421,  0.0432,  0.0643,  0.0759, -0.0600,  0.0563,\n",
       "                      -0.0878,  0.0417, -0.0526,  0.0210,  0.0372, -0.0968]))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence :  list flights from philadelphia to san francisco via dallas\n",
      "Truth        :  O O O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name O B-stoploc.city_name\n",
      "Prediction :  O O O B-fromloc.city_name O B-toloc.city_name <PAD> <PAD> <PAD>\n",
      "Truth        :  atis_flight\n",
      "Prediction :  atis_flight\n"
     ]
    }
   ],
   "source": [
    "index = random.choice(range(len(train_processed)))\n",
    "#index = 0\n",
    "sample = train_raw[index][0]\n",
    "train_in = prepare_sequence(sample,word2index)\n",
    "\n",
    "train_mask = Variable(torch.BoolTensor(tuple(map(lambda s: s ==0, train_in.data)))).view(1,-1)\n",
    "start_decode = Variable(torch.LongTensor([[word2index['<SOS>']]*1])).transpose(1,0)\n",
    "\n",
    "output, hidden_c = encoder(train_in.unsqueeze(0),train_mask.unsqueeze(0))\n",
    "\n",
    "slot_score, intent_score = decoder(start_decode,hidden_c,output,train_mask)\n",
    "\n",
    "v,i = torch.max(slot_score,1)\n",
    "#print(i.data.tolist())\n",
    "print(\"Input Sentence : \",*train_raw[index][0])\n",
    "print(\"Truth        : \",*train_raw[index][1])\n",
    "print(\"Prediction : \",*list(map(lambda ii:index2slot[ii],i.data.tolist())))\n",
    "v,i = torch.max(intent_score,1)\n",
    "print(\"Truth        : \",train_raw[index][2])\n",
    "print(\"Prediction : \",index2intent[i.data.tolist()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(decoder.state_dict(),'model/jointnlu-decoder_05.pkl')\n",
    "torch.save(encoder.state_dict(),'model/jointnlu-encoder_05.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder2 = Encoder(len(word2index),EMBEDDING_SIZE,HIDDEN_SIZE)\n",
    "decoder2 = Decoder(len(slot2index),len(intent2index),len(slot2index)//3,HIDDEN_SIZE*2)\n",
    "\n",
    "encoder2.load_state_dict(torch.load('model/jointnlu-encoder_05.pkl'))\n",
    "decoder2.load_state_dict(torch.load('model/jointnlu-decoder_05.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embedding.weight',\n",
       "              tensor([[ 0.1684, -0.1990,  0.0759,  ..., -0.1788,  0.1600, -0.1141],\n",
       "                      [ 0.0027,  0.0046,  0.0570,  ...,  0.0389,  0.0709, -0.0529],\n",
       "                      [ 0.0329, -0.0408,  0.0381,  ..., -0.0825, -0.0179, -0.0938],\n",
       "                      ...,\n",
       "                      [-0.2382,  0.0178,  0.0556,  ..., -0.1748,  0.1299, -0.1411],\n",
       "                      [ 0.0330, -0.0316, -0.2120,  ...,  0.2453, -0.0847, -0.1772],\n",
       "                      [ 0.1296,  0.0298,  0.0086,  ...,  0.0663,  0.0854,  0.0099]])),\n",
       "             ('lstm.weight_ih_l0',\n",
       "              tensor([[ 0.4226, -0.0424, -0.0424,  ...,  0.2342, -0.1357, -0.0893],\n",
       "                      [ 0.4011, -0.5025,  0.2245,  ...,  0.0035,  0.0680,  0.2268],\n",
       "                      [ 0.6454, -0.1992,  0.1112,  ...,  0.5602, -0.1285, -0.1520],\n",
       "                      ...,\n",
       "                      [ 0.7072, -0.2868,  0.2543,  ...,  0.6171, -0.3054, -0.3099],\n",
       "                      [ 0.5540, -0.4018,  0.3686,  ...,  0.2227, -0.1731, -0.3801],\n",
       "                      [ 0.6534, -0.2252,  0.3509,  ...,  0.3722,  0.1469, -0.2211]])),\n",
       "             ('lstm.weight_hh_l0',\n",
       "              tensor([[ 2.2575e-01,  1.3474e-01,  1.5880e-01,  ..., -2.7249e-01,\n",
       "                        8.2784e-02, -1.4749e-01],\n",
       "                      [ 9.8434e-02,  2.2278e-03,  9.8434e-02,  ..., -1.8808e-01,\n",
       "                        8.4652e-02,  2.0986e-02],\n",
       "                      [-1.1560e-01,  1.2446e-01,  2.3244e-01,  ..., -1.2811e-01,\n",
       "                       -9.8454e-02,  6.3177e-02],\n",
       "                      ...,\n",
       "                      [-1.6085e-04,  2.6345e-01,  1.0086e-01,  ..., -1.9274e-01,\n",
       "                       -7.7750e-03, -8.1993e-02],\n",
       "                      [-4.3103e-02,  2.8017e-01,  2.2839e-01,  ..., -3.1387e-01,\n",
       "                        1.4910e-01,  2.1408e-01],\n",
       "                      [-2.2521e-02,  1.0628e-01,  3.8350e-02,  ..., -5.0207e-02,\n",
       "                       -1.1158e-01,  7.6715e-02]])),\n",
       "             ('lstm.bias_ih_l0',\n",
       "              tensor([ 3.7864e-01,  2.6804e-01,  9.7188e-02,  6.4741e-02,  1.4022e-01,\n",
       "                       2.8789e-01,  1.1649e-01,  9.4842e-02,  1.8246e-01, -9.2548e-03,\n",
       "                       3.6890e-02,  2.0276e-02,  1.9446e-01,  2.3783e-01,  5.4109e-02,\n",
       "                       2.8214e-01,  2.2761e-01,  1.4817e-01,  3.1097e-01,  1.6387e-01,\n",
       "                       7.2814e-02,  1.5508e-01,  2.1493e-01,  2.2111e-01,  3.2159e-01,\n",
       "                       2.1925e-01,  1.9513e-01,  9.2266e-02,  2.5489e-01,  1.6795e-01,\n",
       "                       1.2875e-01,  1.1490e-01,  1.3872e-01,  8.2529e-02, -9.5017e-03,\n",
       "                       2.2070e-01,  6.2542e-02,  9.0279e-02,  1.1368e-01,  1.0924e-01,\n",
       "                       2.3775e-03,  1.3755e-01,  9.8634e-02,  2.4176e-01,  1.8993e-01,\n",
       "                       2.3371e-01,  2.4918e-01, -1.3777e-02,  1.5665e-01, -9.8324e-03,\n",
       "                       1.2275e-01,  1.8610e-01,  1.4770e-01,  5.0702e-01,  1.9454e-01,\n",
       "                       1.4346e-01,  1.8895e-01,  5.0487e-02,  6.1139e-02,  1.1681e-01,\n",
       "                       6.1519e-02,  1.8555e-01,  1.8576e-01,  1.8339e-01,  1.9932e-01,\n",
       "                       6.4918e-02,  1.7751e-01, -3.9031e-02,  2.6935e-03,  8.2153e-02,\n",
       "                       6.8228e-02, -8.7761e-03,  2.0804e-01, -2.5749e-02,  9.8858e-02,\n",
       "                       4.7429e-02,  1.6084e-01,  1.4019e-01,  1.3298e-01,  2.7789e-01,\n",
       "                       1.2920e-01,  1.1750e-01,  2.5693e-01,  8.8421e-02, -3.8762e-02,\n",
       "                       1.1793e-01,  5.1532e-02,  9.5263e-02,  5.9906e-02,  4.4047e-02,\n",
       "                       1.5157e-01,  3.3107e-03, -2.1589e-02,  1.6716e-01,  2.0506e-01,\n",
       "                       1.5800e-01,  6.9752e-02,  1.5832e-01,  2.7236e-02,  2.4567e-01,\n",
       "                       6.4610e-03,  5.6906e-02,  5.8109e-02,  3.9973e-03,  7.3086e-02,\n",
       "                       3.0740e-01,  1.5537e-01,  2.3542e-01, -3.3789e-02,  2.6866e-02,\n",
       "                       8.1048e-02,  1.1271e-01,  8.8649e-02,  9.3383e-02,  1.2447e-02,\n",
       "                      -1.6011e-02,  8.9373e-02,  1.9438e-01,  1.6244e-01,  8.6203e-02,\n",
       "                      -7.0294e-04, -9.2710e-02,  1.1335e-01,  5.5232e-02, -1.0582e-01,\n",
       "                       8.8066e-02,  1.9262e-01,  2.1000e-01,  3.9778e-02, -1.2470e-01,\n",
       "                       1.1047e-01,  1.1652e-01, -7.3228e-02,  1.2724e-01, -5.8429e-02,\n",
       "                       6.2020e-02, -3.1599e-02,  8.9713e-02, -7.0752e-02, -5.6241e-02,\n",
       "                      -2.1325e-03, -1.9188e-01, -4.4312e-02, -2.1350e-02, -7.1115e-02,\n",
       "                       7.4531e-02,  6.2201e-02,  5.4542e-03,  1.1974e-02, -1.1459e-01,\n",
       "                      -8.5994e-02, -4.3499e-02, -8.5272e-02,  9.5380e-02, -1.0570e-02,\n",
       "                      -3.6826e-02,  1.4062e-01, -5.0805e-02,  2.4723e-02, -6.9935e-02,\n",
       "                       1.5026e-01, -5.7383e-02,  7.9475e-03, -1.0124e-01,  8.4443e-02,\n",
       "                      -9.4907e-02, -4.1857e-02, -1.3761e-02,  2.8866e-02, -4.8754e-02,\n",
       "                       4.6477e-02, -1.3235e-02, -9.9784e-02, -3.8958e-02,  1.1323e-02,\n",
       "                      -6.2167e-02,  6.8524e-02,  5.2873e-02, -6.8596e-02,  8.4248e-02,\n",
       "                      -9.1567e-02, -1.3846e-01, -2.3879e-02,  1.7208e-02, -7.1840e-02,\n",
       "                       2.4968e-02,  4.6314e-02,  1.1255e-01,  9.4069e-03, -1.4803e-01,\n",
       "                       8.2700e-02, -1.3118e-01,  4.2390e-01,  1.7231e-01,  1.9989e-01,\n",
       "                       3.8385e-02,  3.2218e-01,  1.0747e-01,  2.8237e-01,  1.1622e-01,\n",
       "                       2.0223e-01,  6.3848e-02,  1.4486e-01, -4.5732e-04,  2.5218e-01,\n",
       "                       3.4264e-01,  5.8964e-02,  3.9075e-01,  2.4606e-01,  4.5496e-02,\n",
       "                       3.9708e-01,  2.8749e-01,  7.0054e-02,  2.5046e-01,  9.8632e-02,\n",
       "                       2.4935e-01,  1.1736e-01,  1.0553e-01,  1.9473e-01, -1.2816e-02,\n",
       "                       1.1616e-01,  4.0752e-01,  2.9384e-01,  9.5155e-02,  4.5662e-02,\n",
       "                       3.4863e-02,  1.7511e-01,  5.4720e-01,  2.5591e-01,  1.4270e-01,\n",
       "                       6.1348e-02,  1.0459e-01,  3.2793e-01,  3.7025e-01,  2.7790e-01,\n",
       "                       1.3676e-01,  9.6302e-02,  1.9051e-01,  1.8006e-01,  1.7320e-01,\n",
       "                       6.8466e-02,  2.2080e-01,  2.1828e-01,  1.6610e-01,  2.6873e-01,\n",
       "                       4.5836e-01,  3.7889e-01,  2.6582e-01,  2.7853e-01,  2.0113e-01,\n",
       "                       5.8218e-02, -9.8935e-04,  1.5585e-01,  1.5194e-01,  3.4866e-01,\n",
       "                       1.9973e-01])),\n",
       "             ('lstm.bias_hh_l0',\n",
       "              tensor([ 0.3711,  0.3265,  0.1612, -0.0298,  0.2058,  0.0909,  0.1238,  0.1150,\n",
       "                       0.1842, -0.0278,  0.0533,  0.2170,  0.1001,  0.2754,  0.0785,  0.2859,\n",
       "                       0.1218,  0.0856,  0.1841,  0.0127,  0.1030,  0.1605, -0.0080,  0.1474,\n",
       "                       0.2219,  0.0794,  0.0312,  0.0686,  0.2689,  0.2685,  0.1783,  0.0572,\n",
       "                       0.0953,  0.0218, -0.0250,  0.3066,  0.0585,  0.1406,  0.1455,  0.1791,\n",
       "                      -0.0183,  0.3484,  0.1639,  0.0646,  0.0254,  0.2383,  0.1196, -0.0450,\n",
       "                       0.1544,  0.0073,  0.1532,  0.2440,  0.2975,  0.4423,  0.2724,  0.0089,\n",
       "                       0.1500, -0.0042,  0.0760,  0.1097,  0.0965,  0.3018,  0.0930,  0.2277,\n",
       "                       0.1052,  0.2561,  0.1693, -0.0296,  0.0970,  0.0682,  0.0058,  0.0852,\n",
       "                       0.1749,  0.0391,  0.1216,  0.1117,  0.0518,  0.1253,  0.0780,  0.2203,\n",
       "                       0.0325, -0.0103,  0.3168,  0.1036,  0.0955,  0.2601,  0.0857,  0.1571,\n",
       "                       0.0603,  0.2180,  0.2063,  0.1261, -0.0594,  0.0931, -0.0332,  0.1489,\n",
       "                       0.1101,  0.1305,  0.0641,  0.2025,  0.0998,  0.0161,  0.1007,  0.1182,\n",
       "                       0.1852,  0.2099,  0.2523,  0.2890, -0.0034,  0.0344,  0.2496,  0.0049,\n",
       "                       0.0892,  0.1563,  0.0034,  0.2250,  0.2288,  0.2583,  0.1376,  0.1873,\n",
       "                       0.2159, -0.0254,  0.0174,  0.1200,  0.0058,  0.1961,  0.1597, -0.0281,\n",
       "                       0.0167,  0.0665,  0.1303,  0.1322,  0.0687, -0.0848, -0.1078, -0.1167,\n",
       "                       0.0709,  0.0575,  0.0593,  0.0166,  0.1016, -0.1503, -0.0633,  0.1339,\n",
       "                       0.0568, -0.0648,  0.0452,  0.0263, -0.0846,  0.0215, -0.0483,  0.0090,\n",
       "                       0.0362,  0.0470,  0.0693,  0.0422,  0.0613,  0.0322, -0.0246, -0.1029,\n",
       "                       0.1217, -0.1008, -0.0406, -0.0484, -0.0626, -0.0104, -0.0977,  0.1084,\n",
       "                       0.0218, -0.0582, -0.0450, -0.0903, -0.0744,  0.1013,  0.0031,  0.0130,\n",
       "                      -0.0068, -0.0608, -0.0224, -0.0372, -0.0205, -0.0054, -0.0531,  0.1399,\n",
       "                       0.0397, -0.1161,  0.0964,  0.1266,  0.0809, -0.1307,  0.0088,  0.0808,\n",
       "                       0.4683,  0.3539,  0.2444,  0.0232,  0.2820,  0.2124,  0.3103,  0.2641,\n",
       "                       0.2778,  0.1457,  0.2521,  0.1530,  0.1929,  0.2946,  0.1745,  0.3179,\n",
       "                       0.3228,  0.1177,  0.2807,  0.0578,  0.2122,  0.1339,  0.1755,  0.2743,\n",
       "                       0.1836,  0.2418,  0.1464,  0.0640,  0.2018,  0.3501,  0.0752,  0.2248,\n",
       "                       0.2295,  0.1616,  0.0013,  0.3549,  0.0358,  0.2547,  0.2162,  0.2177,\n",
       "                       0.3139,  0.4071,  0.3038,  0.2729,  0.1807,  0.1608,  0.3159,  0.1156,\n",
       "                       0.2698,  0.1544,  0.1372,  0.2028,  0.3029,  0.5414,  0.4099,  0.2458,\n",
       "                       0.0803,  0.1741,  0.0843,  0.0775,  0.0600,  0.3698,  0.1686,  0.1763])),\n",
       "             ('lstm.weight_ih_l0_reverse',\n",
       "              tensor([[ 0.1264, -0.1433,  0.0930,  ..., -0.1361,  0.1747, -0.0652],\n",
       "                      [-0.0512,  0.0023, -0.0546,  ..., -0.0307,  0.0568, -0.0432],\n",
       "                      [ 0.1699,  0.0222,  0.0849,  ..., -0.0940,  0.0552,  0.0604],\n",
       "                      ...,\n",
       "                      [-0.3622,  0.4533,  0.2388,  ...,  0.1448, -0.0736,  0.5742],\n",
       "                      [-0.0135,  0.0279, -0.2818,  ...,  0.1766, -0.1267, -0.0569],\n",
       "                      [-0.4516,  0.3391,  0.8146,  ...,  0.3229,  0.0451,  0.3089]])),\n",
       "             ('lstm.weight_hh_l0_reverse',\n",
       "              tensor([[-0.1448, -0.1510, -0.0688,  ..., -0.0148,  0.0465, -0.0520],\n",
       "                      [ 0.0791, -0.1093, -0.0174,  ...,  0.0236,  0.0123, -0.0459],\n",
       "                      [-0.0321,  0.0488,  0.0922,  ..., -0.1721, -0.0663,  0.1199],\n",
       "                      ...,\n",
       "                      [-0.1012, -0.0724, -0.0546,  ...,  0.0460, -0.0361,  0.1330],\n",
       "                      [ 0.0220, -0.1388,  0.0702,  ...,  0.0993, -0.0263, -0.0059],\n",
       "                      [ 0.0539,  0.0499,  0.0124,  ...,  0.0230, -0.1198,  0.0636]])),\n",
       "             ('lstm.bias_ih_l0_reverse',\n",
       "              tensor([ 0.0222,  0.0413, -0.0095,  0.1131,  0.1278,  0.0417, -0.0288,  0.0725,\n",
       "                      -0.0620,  0.0171, -0.0087, -0.0770,  0.0633, -0.0117,  0.3690, -0.0460,\n",
       "                       0.1362,  0.1753,  0.0060,  0.0646, -0.0666,  0.0907, -0.0829,  0.1448,\n",
       "                       0.1281,  0.2704,  0.1054,  0.1550, -0.0961,  0.1671, -0.0882,  0.2726,\n",
       "                       0.2507,  0.0884,  0.1404, -0.0581,  0.0590,  0.1018,  0.0700,  0.1618,\n",
       "                      -0.0520,  0.0078,  0.1512,  0.0436,  0.1154, -0.0197, -0.0452,  0.2731,\n",
       "                      -0.0762,  0.1685,  0.1415, -0.0045,  0.1072,  0.0339, -0.0564,  0.1040,\n",
       "                       0.0279, -0.0360, -0.0770,  0.1306,  0.0519,  0.0894, -0.0820,  0.1320,\n",
       "                      -0.0755,  0.1285,  0.0323, -0.0800,  0.0197,  0.1428,  0.0100,  0.0084,\n",
       "                      -0.0238,  0.0715,  0.0706,  0.1672,  0.0405,  0.0217,  0.1285, -0.0101,\n",
       "                       0.0640,  0.0655,  0.1053,  0.1084,  0.0445,  0.1350,  0.0588, -0.0013,\n",
       "                       0.1308, -0.0548,  0.1002,  0.1450,  0.0166,  0.1527, -0.0415, -0.1219,\n",
       "                       0.0299,  0.0844, -0.0080,  0.0788, -0.0192,  0.1438,  0.0464, -0.0284,\n",
       "                      -0.0673, -0.0178,  0.1083,  0.1135, -0.0293, -0.0207, -0.0076, -0.1328,\n",
       "                      -0.0989,  0.0057, -0.0868,  0.1262, -0.0668,  0.1528,  0.1417,  0.0571,\n",
       "                       0.1470,  0.0128,  0.0020,  0.0087, -0.0688, -0.0112,  0.0664,  0.1362,\n",
       "                      -0.0548,  0.0934, -0.0296,  0.0888,  0.0924, -0.0117,  0.0173, -0.0232,\n",
       "                       0.0209, -0.1328,  0.0362, -0.0492,  0.0261, -0.1316, -0.0573,  0.0933,\n",
       "                       0.0691, -0.0850, -0.0407,  0.1020,  0.0986,  0.0672, -0.0590, -0.1204,\n",
       "                      -0.0169,  0.0476,  0.0806,  0.0381, -0.0030,  0.1151,  0.1409, -0.0030,\n",
       "                       0.0592,  0.1287, -0.0020,  0.0364, -0.0786, -0.0742,  0.0216, -0.0809,\n",
       "                       0.0079,  0.0324, -0.1503,  0.0409,  0.1222, -0.0215,  0.1015, -0.0845,\n",
       "                       0.1400, -0.0972,  0.0522,  0.0586, -0.0946,  0.0777,  0.0041,  0.0384,\n",
       "                      -0.1406, -0.0385,  0.0693,  0.1433, -0.1393, -0.0101, -0.1174,  0.0969,\n",
       "                      -0.0370,  0.0810, -0.0403,  0.1230, -0.0606, -0.0757,  0.0015, -0.0464,\n",
       "                      -0.0705, -0.1095, -0.0176,  0.0639, -0.0361,  0.0174,  0.3251, -0.0306,\n",
       "                       0.0835,  0.1898, -0.0616,  0.0646,  0.0849, -0.0734, -0.0556, -0.0131,\n",
       "                       0.0403,  0.1860,  0.0958, -0.1208,  0.0141,  0.2103,  0.1274,  0.2573,\n",
       "                       0.3195, -0.0720, -0.0300,  0.0286, -0.0158, -0.0757,  0.1205,  0.0859,\n",
       "                      -0.0163,  0.0212,  0.0590,  0.0274,  0.2219, -0.1009, -0.0682,  0.2713,\n",
       "                      -0.0301,  0.1658, -0.0299,  0.0914,  0.1856, -0.0102,  0.0642,  0.1454,\n",
       "                       0.0813, -0.0746,  0.1124,  0.2034, -0.1080, -0.0369,  0.1489, -0.0810])),\n",
       "             ('lstm.bias_hh_l0_reverse',\n",
       "              tensor([ 1.4503e-01,  9.3503e-02,  1.6276e-01,  3.3010e-01, -2.5916e-02,\n",
       "                      -7.0551e-03, -5.1562e-02,  8.5961e-02,  1.5997e-01,  1.3988e-01,\n",
       "                       1.2195e-01, -5.2660e-02,  1.2752e-01,  4.0966e-02,  3.2088e-01,\n",
       "                      -6.2193e-02,  2.1720e-01,  2.1186e-01,  2.9616e-02,  7.5101e-02,\n",
       "                      -1.1496e-02,  8.7714e-02,  5.0709e-02,  4.9443e-02,  7.7999e-02,\n",
       "                       3.7133e-01, -6.1558e-02,  1.5007e-01, -2.0299e-02,  3.3235e-01,\n",
       "                       4.4685e-02,  2.4282e-01,  2.9226e-01, -1.9037e-02,  1.1885e-04,\n",
       "                       1.7213e-01,  8.8657e-02,  1.4894e-01,  2.9139e-01,  1.8984e-01,\n",
       "                       1.5306e-01, -1.7786e-02,  8.9285e-02,  1.3147e-02,  2.0450e-01,\n",
       "                       1.2232e-01,  1.6971e-02,  1.5635e-01,  1.3939e-01,  1.7244e-01,\n",
       "                      -3.3221e-03,  1.7296e-01,  2.0509e-01, -2.9501e-02, -3.9461e-02,\n",
       "                      -2.8769e-02,  7.0118e-02,  4.2357e-02,  2.4565e-02,  1.8886e-01,\n",
       "                       1.6155e-02,  4.7917e-02, -8.0991e-02,  1.4064e-02,  8.7983e-02,\n",
       "                       1.6027e-01,  9.8117e-02, -3.2745e-02,  4.2797e-02, -5.0632e-02,\n",
       "                       1.6476e-01,  5.7500e-02,  4.6879e-02,  3.4127e-02,  6.2835e-02,\n",
       "                      -3.8815e-02, -1.0212e-02,  7.1984e-02, -6.7830e-02,  1.1191e-01,\n",
       "                       9.5180e-02, -1.6946e-02, -3.1889e-03, -9.9204e-02, -1.6958e-02,\n",
       "                      -8.6683e-02,  1.7428e-01, -2.1356e-02, -1.0402e-01,  8.2856e-02,\n",
       "                       3.7592e-02,  7.6622e-02,  2.8547e-02,  8.2876e-02,  9.4465e-02,\n",
       "                       1.0779e-01, -5.6975e-02,  1.3674e-01,  1.1920e-01, -3.4177e-02,\n",
       "                      -9.3747e-02, -5.5303e-02,  1.6269e-01, -7.8135e-02,  1.0531e-01,\n",
       "                       1.1247e-01,  1.0597e-01, -6.8921e-02,  8.2653e-02,  7.0395e-02,\n",
       "                       9.6895e-02, -9.6611e-02,  4.0032e-02,  3.8910e-02,  8.2311e-02,\n",
       "                       1.4210e-01, -6.3577e-03,  1.5937e-01,  6.6238e-02, -5.0464e-02,\n",
       "                      -1.8034e-02,  1.5838e-01,  1.3164e-01,  7.7952e-02,  6.1237e-02,\n",
       "                       1.7863e-01,  8.4517e-02,  8.7683e-02, -1.2248e-01, -1.5550e-01,\n",
       "                       1.3869e-01, -3.5078e-02,  1.1105e-02,  1.2487e-01, -9.5506e-02,\n",
       "                      -1.3998e-01, -1.3447e-01, -3.6841e-02,  7.6083e-02,  8.5399e-02,\n",
       "                       1.1035e-01, -4.3606e-03,  1.5146e-01,  6.1016e-02, -9.0080e-02,\n",
       "                       7.4203e-02, -1.5381e-02,  2.5990e-03, -7.4510e-02,  6.4378e-02,\n",
       "                      -1.2297e-01, -1.1422e-02,  6.1993e-02, -1.0434e-01,  2.9301e-02,\n",
       "                       7.1030e-02,  4.4073e-02, -3.3797e-02,  2.6150e-02,  4.9359e-02,\n",
       "                       4.6619e-02,  2.0317e-02, -1.0535e-01,  8.8444e-02, -1.2445e-01,\n",
       "                       1.1255e-01, -8.4572e-02,  7.4136e-03,  6.6079e-02,  5.8299e-02,\n",
       "                      -1.3942e-02, -1.6353e-01, -3.3532e-02, -1.1275e-01,  2.3613e-02,\n",
       "                      -6.0701e-02,  1.0314e-01, -4.9307e-02,  1.3032e-01,  9.6648e-02,\n",
       "                       1.1853e-01, -3.3065e-02,  5.4646e-02, -1.2220e-01, -1.5059e-01,\n",
       "                      -8.7195e-02,  8.4678e-02, -5.4129e-02, -1.1441e-01, -4.5103e-02,\n",
       "                      -1.3932e-01, -1.8371e-02, -3.8908e-02, -5.5046e-02, -2.7101e-02,\n",
       "                       9.0378e-02, -1.0335e-01,  5.0626e-02,  1.0372e-01, -1.3587e-02,\n",
       "                       1.4146e-02,  3.2455e-02,  8.8192e-02, -9.0804e-03,  1.2571e-01,\n",
       "                       1.2348e-01,  1.8872e-01, -4.3822e-02,  1.7328e-01,  7.0846e-02,\n",
       "                       1.6623e-01,  1.1095e-01,  1.3710e-01, -3.2480e-02, -4.7362e-02,\n",
       "                       8.7546e-02,  1.9714e-01,  2.8551e-01, -4.4140e-02, -1.0995e-01,\n",
       "                      -5.8498e-02,  2.3198e-01,  7.8149e-02,  8.8168e-02,  2.7771e-01,\n",
       "                       7.4517e-02, -7.5165e-03,  3.5035e-02,  1.2220e-01, -6.6915e-02,\n",
       "                       1.9523e-01, -1.6543e-02, -4.4177e-02,  3.0372e-02, -5.2318e-03,\n",
       "                      -1.1111e-02,  1.3533e-01, -7.1206e-02,  2.9436e-03,  1.0576e-01,\n",
       "                       3.0171e-02,  3.5127e-01,  1.4465e-01,  2.3013e-02,  1.9621e-01,\n",
       "                       4.9517e-02,  1.4417e-01,  1.7374e-01,  7.7033e-02, -7.0552e-03,\n",
       "                       4.0574e-02,  2.3211e-01,  1.2483e-01,  7.1559e-02,  1.0039e-01,\n",
       "                       5.7219e-03]))])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embedding.weight',\n",
       "              tensor([[ 0.0691, -0.1286,  0.1308,  ..., -0.0307,  0.0991, -0.0579],\n",
       "                      [-0.0789, -0.0999, -0.0474,  ..., -0.0974, -0.0409, -0.0069],\n",
       "                      [ 0.3179, -0.1235, -0.0877,  ...,  0.5900,  0.1240,  0.3409],\n",
       "                      ...,\n",
       "                      [ 0.3077, -0.4131,  0.3278,  ..., -0.3472,  0.3740, -0.3759],\n",
       "                      [-0.0246,  0.0926, -0.0192,  ...,  0.0010, -0.0955, -0.0400],\n",
       "                      [ 0.2618, -0.3421,  0.2751,  ..., -0.6791,  0.0608, -0.5264]])),\n",
       "             ('lstm.weight_ih_l0',\n",
       "              tensor([[ 0.0087, -0.0722,  0.0966,  ...,  0.0225, -0.1046,  0.0277],\n",
       "                      [ 0.0574, -0.0525,  0.0890,  ..., -0.0577, -0.0264,  0.1666],\n",
       "                      [ 0.0287, -0.0366,  0.0901,  ..., -0.0104, -0.0711,  0.0359],\n",
       "                      ...,\n",
       "                      [ 0.1209, -0.1646,  0.2160,  ...,  0.3461,  0.0481, -0.4790],\n",
       "                      [ 0.0595, -0.0660, -0.0052,  ...,  0.0233, -0.0250, -0.0026],\n",
       "                      [-0.0321, -0.0380,  0.0360,  ..., -0.0629, -0.0244,  0.3845]])),\n",
       "             ('lstm.weight_hh_l0',\n",
       "              tensor([[ 0.0917, -0.0012,  0.0947,  ..., -0.0555,  0.1039,  0.0811],\n",
       "                      [ 0.0561, -0.0083, -0.0201,  ..., -0.0225, -0.0588, -0.0440],\n",
       "                      [ 0.0783, -0.0819,  0.0755,  ...,  0.0365, -0.0050, -0.0114],\n",
       "                      ...,\n",
       "                      [ 0.0591, -0.1491,  0.0516,  ..., -0.0101,  0.1036,  0.2021],\n",
       "                      [ 0.0958, -0.1195,  0.0781,  ..., -0.0924,  0.0789,  0.1128],\n",
       "                      [-0.0192,  0.0696, -0.0710,  ...,  0.0419,  0.0698,  0.0672]])),\n",
       "             ('lstm.bias_ih_l0',\n",
       "              tensor([-0.0062,  0.0888,  0.0091,  0.0772, -0.0563,  0.0564,  0.0592,  0.0404,\n",
       "                      -0.0397, -0.0080,  0.0597,  0.0891,  0.0527,  0.0429,  0.0304,  0.0362,\n",
       "                      -0.0537, -0.0079, -0.0234,  0.0479, -0.0148, -0.0102,  0.0442,  0.0972,\n",
       "                       0.0467, -0.0583, -0.0018,  0.0547,  0.0674,  0.0049,  0.0493,  0.0956,\n",
       "                       0.0593,  0.0380,  0.0496,  0.0289,  0.0193,  0.0142,  0.0947, -0.0322,\n",
       "                       0.0323,  0.0264, -0.0582, -0.0312,  0.1243, -0.0259,  0.0831,  0.1177,\n",
       "                       0.0168,  0.1221, -0.0037,  0.0315,  0.0411, -0.0061, -0.0404,  0.0414,\n",
       "                       0.0672, -0.0248,  0.0104,  0.0392,  0.0423,  0.0585,  0.0927, -0.0118,\n",
       "                      -0.0200, -0.0308,  0.0679,  0.1101, -0.0174, -0.0232, -0.0046, -0.0127,\n",
       "                       0.0814,  0.0951, -0.0335,  0.0619,  0.1359,  0.0239,  0.0114,  0.0520,\n",
       "                       0.0299,  0.0498,  0.0288,  0.1879,  0.0628,  0.0722,  0.0869,  0.1036,\n",
       "                       0.0395,  0.0160, -0.0543,  0.1080,  0.0219, -0.0082, -0.0574, -0.0288,\n",
       "                      -0.0115,  0.0607, -0.0180, -0.0537, -0.0214,  0.0025, -0.0079,  0.0902,\n",
       "                       0.0652,  0.0692,  0.0120,  0.0548, -0.0227,  0.0243,  0.1062, -0.0272,\n",
       "                      -0.0568,  0.0149,  0.1411, -0.0199, -0.0523, -0.0577,  0.1082, -0.0403,\n",
       "                       0.1119,  0.0803,  0.0153,  0.0557,  0.0264,  0.0634,  0.1132,  0.0329,\n",
       "                       0.0915,  0.0743,  0.0060,  0.0079,  0.0122,  0.0340,  0.0878, -0.0203,\n",
       "                       0.0116, -0.0323,  0.0771,  0.0231,  0.0490,  0.0729,  0.0702, -0.0217,\n",
       "                       0.0246, -0.0294,  0.0932, -0.0421, -0.0487,  0.0551,  0.0498,  0.0048,\n",
       "                      -0.0239,  0.0529,  0.0635, -0.0057,  0.0563, -0.0156,  0.1033, -0.0301,\n",
       "                       0.1078,  0.0287,  0.0489,  0.0562,  0.0400,  0.0003, -0.0820,  0.0640,\n",
       "                      -0.0142, -0.0326,  0.0107,  0.0583,  0.0003,  0.0923,  0.0444, -0.0296,\n",
       "                       0.0123, -0.0113,  0.0857,  0.0766,  0.1128,  0.1105,  0.0294,  0.0342,\n",
       "                       0.0610, -0.0357,  0.1006, -0.0454,  0.0547, -0.0406,  0.1030, -0.0143,\n",
       "                       0.0900,  0.1014,  0.0915, -0.0469, -0.0655,  0.0953, -0.0830,  0.0420,\n",
       "                      -0.0046, -0.0611,  0.0664,  0.0243,  0.0641, -0.0088,  0.0178, -0.0307,\n",
       "                       0.0234,  0.1023, -0.0267,  0.1058, -0.0126, -0.0200,  0.0310, -0.0745,\n",
       "                      -0.0033,  0.0386, -0.0012,  0.0188, -0.0526,  0.0981, -0.0091, -0.0509,\n",
       "                       0.0781, -0.0321,  0.0276,  0.0982, -0.0733,  0.0332,  0.0333, -0.0024,\n",
       "                      -0.0493,  0.0440,  0.0288, -0.0761,  0.0701,  0.1006,  0.1016,  0.0633,\n",
       "                      -0.0460,  0.0269, -0.0831, -0.0163,  0.0752,  0.0217, -0.0587, -0.0242,\n",
       "                      -0.0318, -0.0020, -0.0397,  0.0398,  0.0043,  0.0954, -0.0202,  0.0945,\n",
       "                       0.0418,  0.0136,  0.1038, -0.0255, -0.0017,  0.1080,  0.0427, -0.0032,\n",
       "                       0.1092, -0.0762, -0.0525, -0.0196,  0.0707, -0.0172, -0.0877,  0.0377,\n",
       "                      -0.0065, -0.0022, -0.0189,  0.0478, -0.0292,  0.0112,  0.0897, -0.0147,\n",
       "                      -0.0371, -0.0521,  0.0798,  0.0736, -0.1064, -0.0009,  0.0771,  0.0260,\n",
       "                      -0.0142, -0.0838,  0.0322,  0.0502, -0.0401,  0.0926,  0.0203, -0.0691,\n",
       "                       0.0482, -0.0162, -0.1111, -0.0043,  0.0427,  0.0766,  0.0235,  0.0229,\n",
       "                      -0.0062,  0.0542, -0.0190, -0.0833,  0.0731, -0.0436, -0.0602,  0.0918,\n",
       "                       0.0456, -0.0942,  0.0343, -0.0481, -0.0925, -0.0197,  0.0229,  0.1117,\n",
       "                       0.0060,  0.0995,  0.0233,  0.0213, -0.0442, -0.0113,  0.0176, -0.0726,\n",
       "                      -0.0350,  0.0530,  0.0273,  0.0201, -0.0041,  0.0776, -0.0447,  0.0050,\n",
       "                      -0.0929, -0.0603, -0.0083,  0.0721,  0.1133,  0.1098, -0.0370, -0.0800,\n",
       "                       0.0121, -0.0387,  0.0737,  0.0446,  0.0239, -0.0562, -0.0934, -0.0845,\n",
       "                       0.0636, -0.0319,  0.0672, -0.0092,  0.0500, -0.0502,  0.0350,  0.0559,\n",
       "                       0.0854,  0.0619, -0.0763,  0.0358, -0.0758,  0.0887,  0.0627, -0.0801,\n",
       "                       0.0383, -0.0121,  0.0033,  0.0295, -0.0936, -0.0020, -0.0169,  0.0081,\n",
       "                      -0.0552,  0.0447,  0.0744, -0.0983, -0.0549, -0.0198, -0.0216,  0.0675,\n",
       "                      -0.0248,  0.0352,  0.0906,  0.0294,  0.0957,  0.0311,  0.0864, -0.0468,\n",
       "                       0.0593,  0.1103,  0.0592, -0.0153, -0.0404, -0.0563, -0.0251,  0.0521,\n",
       "                      -0.0353, -0.0164,  0.1388, -0.0759,  0.0149,  0.0887, -0.0049, -0.0131,\n",
       "                      -0.0117, -0.0048,  0.0720,  0.0193,  0.0038,  0.0310,  0.0499,  0.1356,\n",
       "                      -0.0669,  0.0236,  0.0120,  0.0193,  0.0524,  0.1313,  0.0743,  0.0170,\n",
       "                       0.0461,  0.0043, -0.0839,  0.0254,  0.0684,  0.0728,  0.1004,  0.1207,\n",
       "                      -0.0159,  0.0632, -0.0373, -0.0446,  0.1182,  0.0447,  0.0815,  0.0799,\n",
       "                      -0.0768,  0.0750,  0.0775, -0.0459,  0.0755, -0.0128, -0.0437, -0.0508,\n",
       "                       0.0002,  0.0254, -0.0666,  0.0672, -0.0342, -0.0454, -0.0097,  0.0812,\n",
       "                       0.0141,  0.0120,  0.0784,  0.0035,  0.0883, -0.0257,  0.0473, -0.1278,\n",
       "                       0.0034,  0.0485, -0.0194,  0.1557, -0.0200,  0.0714,  0.0213, -0.1256,\n",
       "                      -0.0646,  0.0915,  0.1019,  0.0501,  0.1085,  0.1003,  0.0166, -0.0338,\n",
       "                       0.0992,  0.0389,  0.0480,  0.0424, -0.0560,  0.0965,  0.0992, -0.0423,\n",
       "                      -0.0472,  0.0246,  0.0482,  0.0905, -0.0480,  0.0699,  0.0489,  0.0061,\n",
       "                      -0.0709,  0.0296,  0.0695, -0.0038,  0.0704, -0.0713,  0.0857, -0.0730,\n",
       "                       0.0652,  0.0576,  0.0044,  0.0142, -0.0219, -0.0105,  0.0543, -0.0214])),\n",
       "             ('lstm.bias_hh_l0',\n",
       "              tensor([ 6.7375e-02, -6.0628e-02,  8.7502e-02,  9.3736e-02,  3.1443e-02,\n",
       "                       2.6047e-02,  1.0370e-01, -5.4829e-03,  4.9925e-02,  6.1939e-02,\n",
       "                       8.1827e-02,  3.7215e-02,  3.9510e-02,  1.0800e-01,  7.6906e-02,\n",
       "                      -6.6409e-03,  6.3096e-02,  4.3977e-03, -4.6048e-02,  6.4845e-02,\n",
       "                      -5.0045e-02,  1.7447e-02, -4.1833e-02, -4.2072e-02, -3.6019e-02,\n",
       "                       5.8870e-02,  9.5454e-02,  6.7904e-02, -6.1910e-03,  5.4690e-02,\n",
       "                       8.4480e-03,  1.3426e-01, -3.7640e-02, -4.7434e-02,  7.0245e-02,\n",
       "                      -1.6138e-02,  6.8524e-02, -2.0404e-02,  1.0810e-01, -4.5373e-02,\n",
       "                       1.0983e-01, -5.6865e-02, -2.1650e-02,  8.8873e-02,  1.2025e-01,\n",
       "                       4.3907e-02,  1.2197e-01,  1.5511e-01,  9.2700e-02, -2.9263e-02,\n",
       "                       1.0983e-01,  4.5897e-02,  2.8287e-02,  9.0935e-02,  4.3983e-02,\n",
       "                      -4.5043e-02, -5.1166e-02, -2.2749e-04, -4.7912e-02,  6.5427e-02,\n",
       "                       9.5099e-02,  7.7893e-03,  7.9509e-02, -5.8412e-02,  9.5753e-02,\n",
       "                      -6.3107e-02,  1.1693e-01, -5.3544e-03,  8.6336e-02, -3.6996e-02,\n",
       "                       1.2546e-01,  8.9040e-02, -1.1881e-02,  7.1866e-02, -5.0029e-02,\n",
       "                       8.4030e-02,  2.3687e-02,  6.1852e-02, -3.3093e-02,  2.4347e-02,\n",
       "                       5.8900e-02,  4.9026e-02, -3.4523e-02,  1.6594e-01,  9.6252e-02,\n",
       "                      -4.4567e-02,  6.8595e-02,  5.5338e-02, -4.0323e-02,  8.3380e-02,\n",
       "                       4.2234e-03,  9.6755e-02, -1.1073e-02,  3.4663e-02, -6.2432e-02,\n",
       "                       8.3152e-02,  5.5132e-02, -1.4239e-02,  3.3000e-03, -1.2549e-02,\n",
       "                       9.4069e-02,  1.1079e-01,  8.6632e-03,  7.6512e-02,  6.8944e-02,\n",
       "                      -3.4775e-02,  7.0072e-02,  3.4838e-02,  6.2021e-02,  5.2840e-02,\n",
       "                      -2.0126e-02,  1.0797e-01,  5.1328e-02,  7.8883e-02,  6.7414e-03,\n",
       "                       8.8573e-02,  5.0987e-02,  4.2586e-02,  7.8992e-03,  2.6106e-04,\n",
       "                       3.2938e-02,  9.5844e-02,  1.9741e-02, -2.9998e-02,  4.1093e-02,\n",
       "                       3.3035e-02, -3.2380e-02, -2.4055e-02, -2.5706e-02,  2.9961e-02,\n",
       "                       9.6877e-02, -1.5423e-02,  7.6233e-02,  1.9513e-03,  6.8110e-02,\n",
       "                      -5.4429e-02, -4.1603e-02,  7.0540e-02, -4.4845e-02, -2.8104e-02,\n",
       "                      -2.4922e-02,  5.1748e-02, -4.4966e-02,  6.4572e-02,  7.5952e-02,\n",
       "                       3.1233e-02,  5.1464e-02,  7.5349e-02,  4.0878e-03,  6.3932e-02,\n",
       "                       8.5822e-02, -3.9782e-02,  4.0157e-02,  9.4247e-02,  2.3956e-02,\n",
       "                       9.7014e-03, -4.6732e-02,  7.0457e-02,  8.2366e-02,  8.9731e-02,\n",
       "                       6.7169e-02,  2.6570e-02,  4.0823e-02,  1.0105e-01,  4.0999e-02,\n",
       "                       1.0599e-01, -4.0883e-02, -6.3569e-02, -3.8788e-02,  3.2419e-02,\n",
       "                      -1.8191e-02, -6.9815e-03,  3.9805e-02, -5.3520e-02, -3.2665e-02,\n",
       "                      -2.2734e-02, -2.5435e-02,  1.0570e-01,  1.4667e-02,  1.5732e-02,\n",
       "                       7.5908e-02,  8.7780e-02, -8.7748e-02,  1.0336e-01,  1.0965e-01,\n",
       "                       1.3013e-02,  8.3907e-02, -5.9986e-02, -2.6238e-02, -5.0280e-02,\n",
       "                       1.8811e-04,  8.2220e-02,  1.0411e-01, -2.5209e-02, -5.0731e-02,\n",
       "                       2.5552e-02, -8.1791e-02, -5.4458e-02,  3.1733e-02, -1.5067e-02,\n",
       "                      -4.2597e-03, -5.5121e-02, -1.4793e-04, -2.2729e-02, -6.4982e-02,\n",
       "                       9.7857e-03, -3.0389e-02,  7.3702e-02, -2.6831e-02, -6.9968e-03,\n",
       "                      -7.1262e-03,  7.1878e-02,  2.1066e-03,  8.6436e-02,  1.0877e-02,\n",
       "                       4.1897e-02, -6.0925e-02, -4.9047e-02,  1.1671e-02,  4.5965e-02,\n",
       "                       1.5466e-02,  9.8103e-02, -1.3611e-02,  9.9707e-02, -2.4101e-02,\n",
       "                       5.5578e-03,  3.2675e-02, -1.5580e-02,  2.4336e-02,  7.2661e-02,\n",
       "                       1.0560e-01,  1.1099e-02, -4.3873e-02,  1.0685e-02,  4.3380e-02,\n",
       "                      -2.0260e-02,  8.7606e-03, -3.5788e-02,  4.3908e-02,  6.9335e-02,\n",
       "                       1.0090e-01,  7.1052e-02, -8.1460e-02, -3.3838e-02,  6.2742e-02,\n",
       "                       2.9153e-02,  9.0361e-02, -3.2169e-02, -5.5794e-02, -2.6179e-02,\n",
       "                      -6.3112e-02,  5.6289e-02,  7.1000e-02, -1.8635e-02, -6.8965e-03,\n",
       "                      -2.0091e-02, -4.4336e-03, -1.1384e-01,  3.3215e-02, -5.6995e-02,\n",
       "                       3.2470e-02,  1.5508e-02,  8.2406e-02,  9.9058e-02,  9.0074e-02,\n",
       "                      -8.4986e-02,  2.0138e-03,  8.9102e-03,  1.1336e-01,  8.9407e-02,\n",
       "                      -8.4802e-02, -2.1940e-02, -9.2540e-02,  8.1093e-02, -9.3686e-03,\n",
       "                       1.0199e-01, -5.5159e-02, -1.1919e-02,  1.0889e-01,  4.5121e-02,\n",
       "                      -1.0807e-01,  6.2496e-02, -3.1164e-02,  5.9282e-02, -4.4988e-02,\n",
       "                       6.2870e-02,  6.0825e-02,  9.2163e-02, -9.9985e-02, -3.3780e-02,\n",
       "                      -3.4934e-02, -1.5072e-02, -1.8116e-03,  5.0992e-02,  4.5897e-02,\n",
       "                      -5.0574e-03,  4.3463e-02, -5.3009e-02, -8.9743e-02,  3.4928e-02,\n",
       "                      -2.9386e-02,  2.9218e-02,  6.6720e-02,  6.4105e-02,  7.4515e-02,\n",
       "                      -6.3380e-02,  3.7469e-02,  2.1374e-02,  9.0284e-02,  1.0561e-01,\n",
       "                       2.2740e-02,  9.7578e-02,  3.1535e-02,  1.9968e-02, -8.5756e-02,\n",
       "                       1.0886e-01, -8.3635e-02, -6.7937e-02, -3.2800e-02,  1.0516e-01,\n",
       "                      -3.7525e-02, -1.5570e-03,  2.4860e-02, -6.6527e-02,  9.5752e-03,\n",
       "                      -2.8589e-02,  3.1715e-02,  6.2953e-03, -8.4395e-03,  5.4305e-02,\n",
       "                      -5.2772e-02, -9.7442e-03,  3.5638e-03,  4.0642e-02, -1.5015e-02,\n",
       "                      -4.2813e-03, -7.7740e-02, -1.0964e-01, -5.9355e-03,  9.5396e-04,\n",
       "                      -4.4064e-03,  9.0410e-02,  1.5563e-02, -8.0245e-02, -6.3029e-02,\n",
       "                      -7.1832e-03,  2.3479e-02, -1.5962e-02, -2.8091e-02,  5.7309e-02,\n",
       "                      -8.4055e-02, -5.6057e-02, -1.0583e-01,  5.2679e-02,  1.4315e-02,\n",
       "                       2.0195e-02,  2.9726e-02,  3.6882e-02, -1.7377e-02,  3.5309e-02,\n",
       "                      -1.6354e-02,  1.0153e-01, -6.9545e-02,  6.7534e-02, -1.9995e-02,\n",
       "                      -4.3330e-02,  1.0816e-01,  9.5681e-04, -2.3772e-02,  1.1342e-01,\n",
       "                       1.2892e-02, -7.6472e-03,  2.5519e-02, -1.0072e-01,  9.4870e-02,\n",
       "                       1.0210e-01, -3.2336e-02,  6.7986e-02, -2.5226e-02, -7.7351e-02,\n",
       "                       5.6170e-02, -3.9855e-02, -3.7899e-02,  9.9367e-02,  6.8568e-02,\n",
       "                       1.0743e-01,  5.4532e-02, -1.7975e-02,  6.4154e-02,  3.8490e-02,\n",
       "                      -1.3863e-02, -3.6615e-02, -7.2620e-02,  9.9073e-02,  1.4790e-01,\n",
       "                      -2.3748e-02, -6.8770e-02,  2.4858e-03,  1.8620e-03,  1.1195e-01,\n",
       "                       2.2901e-02,  1.7269e-03,  2.4037e-02, -6.7683e-02, -1.2945e-01,\n",
       "                       5.3454e-02, -2.9311e-02, -4.4230e-02,  6.5893e-02, -1.0487e-01,\n",
       "                       5.5676e-02,  3.5706e-02, -3.9560e-02, -1.2021e-02,  4.5228e-02,\n",
       "                       1.5021e-01,  4.8596e-02, -2.3234e-02,  1.5097e-03, -3.7606e-02,\n",
       "                       4.0459e-02,  1.2934e-01,  8.0002e-02,  1.4261e-02, -5.6485e-02,\n",
       "                      -9.8102e-02,  2.5860e-02,  4.8493e-02,  1.1797e-01, -4.5707e-02,\n",
       "                       5.4918e-02, -1.1619e-02,  2.1925e-03,  1.5006e-02,  5.6446e-03,\n",
       "                       7.1375e-03,  9.0989e-02,  8.7065e-02,  6.6777e-02,  3.4013e-02,\n",
       "                      -1.3232e-02, -8.0744e-02,  3.7339e-02,  8.4059e-02,  2.8175e-02,\n",
       "                       6.7659e-02,  7.0859e-03, -2.3162e-03,  1.1583e-01, -5.7497e-02,\n",
       "                       3.4626e-02,  3.6588e-02,  4.7142e-02,  1.4055e-02,  2.9344e-02,\n",
       "                       6.1371e-03, -5.2735e-02,  3.0621e-02,  6.7630e-02,  4.1379e-02,\n",
       "                       2.1019e-02,  4.2884e-02, -6.6598e-02, -3.2435e-02,  7.4930e-02,\n",
       "                       1.9728e-03,  1.7281e-02,  1.9597e-02,  3.5206e-02,  6.4896e-02,\n",
       "                       4.9078e-02, -8.4995e-02, -4.2955e-02,  1.0259e-01,  1.0383e-01,\n",
       "                       3.6248e-02, -4.7080e-02,  8.1545e-02,  2.4879e-02,  6.9297e-02,\n",
       "                       1.1157e-01,  4.2946e-02,  5.8295e-02,  3.1718e-02, -1.0599e-02,\n",
       "                       7.2571e-03, -1.2049e-02,  8.1660e-03,  1.3574e-02, -4.4103e-02,\n",
       "                      -1.8780e-02,  1.0717e-01,  4.6067e-02,  1.6078e-02, -4.1261e-02,\n",
       "                       8.5571e-02,  4.3557e-02,  1.7172e-02,  1.1082e-02,  4.1292e-02,\n",
       "                       2.4376e-02, -8.6666e-02, -3.6332e-02, -1.1496e-01, -3.8267e-02,\n",
       "                       7.9196e-02, -5.3152e-02,  3.0039e-02, -3.3557e-02,  3.1197e-02,\n",
       "                      -3.3644e-02,  6.6820e-02])),\n",
       "             ('attn.weight',\n",
       "              tensor([[-0.0884,  0.0486,  0.0602,  ...,  0.1030,  0.1411, -0.0920],\n",
       "                      [-0.0502, -0.1158,  0.0170,  ..., -0.1083, -0.0189,  0.1830],\n",
       "                      [-0.1403,  0.0792, -0.0666,  ...,  0.0430, -0.0110,  0.0374],\n",
       "                      ...,\n",
       "                      [ 0.0078,  0.0395, -0.1073,  ...,  0.0445, -0.0016, -0.0282],\n",
       "                      [-0.1077, -0.0487,  0.1383,  ...,  0.1820,  0.0395, -0.0247],\n",
       "                      [-0.0798,  0.1385,  0.0956,  ..., -0.0913,  0.1120, -0.0295]])),\n",
       "             ('attn.bias',\n",
       "              tensor([ 0.0303, -0.0048,  0.0380,  0.0369, -0.0162,  0.0237,  0.0670,  0.0293,\n",
       "                       0.0334, -0.0459, -0.0831, -0.0836, -0.0134, -0.0767,  0.0112,  0.0038,\n",
       "                      -0.0281, -0.0111, -0.0613, -0.0240,  0.0082, -0.0716, -0.0173,  0.0006,\n",
       "                      -0.0617, -0.0083, -0.0100, -0.0099, -0.0559, -0.0330, -0.0685,  0.0828,\n",
       "                      -0.0428,  0.0626, -0.0336,  0.0098,  0.0181,  0.0766,  0.0745, -0.0372,\n",
       "                       0.0209, -0.0572,  0.0068,  0.0322,  0.0780, -0.0436,  0.0012, -0.0326,\n",
       "                      -0.0349, -0.0865, -0.0796,  0.0738, -0.0481, -0.0809,  0.0250, -0.0465,\n",
       "                      -0.0606,  0.0576,  0.0651, -0.0798, -0.0173, -0.0694,  0.0120, -0.0205,\n",
       "                       0.0824,  0.0856,  0.0421, -0.0629,  0.0816,  0.0362,  0.0684, -0.0024,\n",
       "                       0.0806, -0.0261, -0.0829,  0.0488, -0.0010, -0.0031, -0.0202, -0.0555,\n",
       "                      -0.0300,  0.0551,  0.0331,  0.0342, -0.0619,  0.0671, -0.0542, -0.0548,\n",
       "                      -0.0168,  0.0128, -0.0397, -0.0653, -0.0631,  0.0570, -0.0498, -0.0705,\n",
       "                       0.0367, -0.0660, -0.0678, -0.0844,  0.0017,  0.0802, -0.0180, -0.0289,\n",
       "                      -0.0761, -0.0856,  0.0652, -0.0141, -0.0103,  0.0269,  0.0085, -0.0631,\n",
       "                       0.0840,  0.0560,  0.0479, -0.0383, -0.0647,  0.0725,  0.0314,  0.0361,\n",
       "                       0.0211,  0.0081,  0.0539,  0.0121, -0.0127,  0.0110,  0.0204, -0.0486])),\n",
       "             ('slot_out.weight',\n",
       "              tensor([[ 0.1736, -0.0749,  0.0532,  ...,  0.0880, -0.0213,  0.0013],\n",
       "                      [-0.0768,  0.0246, -0.0902,  ...,  0.1696,  0.1270, -0.1485],\n",
       "                      [-0.0931,  0.0469, -0.0718,  ...,  0.1277,  0.0489, -0.0089],\n",
       "                      ...,\n",
       "                      [-0.1802, -0.1007,  0.2063,  ..., -0.0770,  0.0308, -0.0961],\n",
       "                      [ 0.0648,  0.0145,  0.0156,  ..., -0.0557, -0.0247, -0.0028],\n",
       "                      [-0.2295,  0.0431,  0.2679,  ...,  0.0437, -0.0482, -0.1704]])),\n",
       "             ('slot_out.bias',\n",
       "              tensor([ 0.0583, -0.0837, -0.0457,  0.0392,  0.0288, -0.0519, -0.0468,  0.0162,\n",
       "                       0.0283,  0.0369,  0.0012, -0.0409,  0.0134, -0.0119,  0.0316, -0.0734,\n",
       "                       0.0129, -0.0525, -0.0420, -0.0175,  0.0449, -0.0406,  0.0141, -0.0111,\n",
       "                      -0.0474, -0.0252, -0.0708,  0.0354,  0.0394, -0.0288,  0.0179, -0.0240,\n",
       "                       0.0714, -0.0181, -0.0611, -0.0646,  0.0374, -0.0036, -0.0216,  0.0556,\n",
       "                      -0.0354, -0.0450, -0.0593,  0.0079, -0.0643,  0.0214,  0.0087,  0.0700,\n",
       "                      -0.0387, -0.0544, -0.0348, -0.0252, -0.0461,  0.0125, -0.0467, -0.0224,\n",
       "                      -0.0472, -0.0422, -0.0483,  0.0309, -0.0360, -0.0520,  0.0385,  0.0198,\n",
       "                      -0.0361, -0.0485, -0.0041,  0.0309, -0.1112, -0.0553, -0.0850, -0.0358,\n",
       "                      -0.0118, -0.0434, -0.0654, -0.0354, -0.0865, -0.0664, -0.0546,  0.0452,\n",
       "                      -0.0569,  0.0195, -0.0094, -0.0960, -0.0489, -0.0673, -0.0482, -0.0622,\n",
       "                      -0.0050, -0.0049, -0.0521,  0.0146,  0.0286,  0.0413, -0.0222,  0.0482,\n",
       "                      -0.0005, -0.0141, -0.0440, -0.0141,  0.0185,  0.0262, -0.0002, -0.0498,\n",
       "                      -0.0840, -0.0291, -0.0461,  0.0186, -0.0228,  0.0166,  0.0100, -0.0813,\n",
       "                       0.0203, -0.0083,  0.0353,  0.0058,  0.0147,  0.0500, -0.0738, -0.0646,\n",
       "                       0.0221,  0.0593])),\n",
       "             ('intent_out.weight',\n",
       "              tensor([[-0.0206,  0.0311, -0.0248,  ...,  0.2712,  0.2496, -0.1072],\n",
       "                      [ 0.0003,  0.0470, -0.0222,  ...,  0.0368, -0.0347, -0.0945],\n",
       "                      [-0.0122, -0.0372, -0.0489,  ...,  0.1128,  0.1197, -0.0506],\n",
       "                      ...,\n",
       "                      [-0.0551,  0.0491, -0.0474,  ...,  0.0303,  0.0936,  0.0234],\n",
       "                      [-0.0390,  0.0521, -0.0555,  ..., -0.0192,  0.0490,  0.0463],\n",
       "                      [ 0.0235, -0.0200,  0.0577,  ...,  0.1031,  0.1504, -0.1151]])),\n",
       "             ('intent_out.bias',\n",
       "              tensor([-0.0316, -0.0701, -0.0211,  0.0218, -0.0749,  0.0459,  0.0634, -0.0515,\n",
       "                      -0.1219, -0.0069,  0.0421,  0.0432,  0.0643,  0.0759, -0.0600,  0.0563,\n",
       "                      -0.0878,  0.0417, -0.0526,  0.0210,  0.0372, -0.0968]))])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62, 62, 62, 62, 48, 62, 57, 0, 0, 0, 0]\n",
      "Input Sentence :  which airlines fly from boston to washington dc via other cities\n",
      "Truth        :  O O O O B-fromloc.city_name O B-toloc.city_name B-toloc.state_code O O O\n",
      "Prediction :  O O O O B-fromloc.city_name O B-toloc.city_name <PAD> <PAD> <PAD> <PAD>\n",
      "Truth        :  atis_airline\n",
      "Prediction :  atis_airline\n"
     ]
    }
   ],
   "source": [
    "index = 4\n",
    "sample = train_raw[index][0]\n",
    "train_in = prepare_sequence(sample,word2index)\n",
    "\n",
    "train_mask = Variable(torch.BoolTensor(tuple(map(lambda s: s ==0, train_in.data)))).view(1,-1)\n",
    "start_decode = Variable(torch.LongTensor([[word2index['<SOS>']]*1])).transpose(1,0)\n",
    "\n",
    "output, hidden_c = encoder2(train_in.unsqueeze(0),train_mask.unsqueeze(0))\n",
    "\n",
    "slot_score, intent_score = decoder2(start_decode,hidden_c,output,train_mask)\n",
    "\n",
    "v,i = torch.max(slot_score,1)\n",
    "print(i.data.tolist())\n",
    "print(\"Input Sentence : \",*train_raw[index][0])\n",
    "print(\"Truth        : \",*train_raw[index][1])\n",
    "print(\"Prediction : \",*list(map(lambda ii:index2slot[ii],i.data.tolist())))\n",
    "v,i = torch.max(intent_score,1)\n",
    "print(\"Truth        : \",train_raw[index][2])\n",
    "print(\"Prediction : \",index2intent[i.data.tolist()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw = load_data(\"dataset/atis-2.dev.w-intent.iob\")\n",
    "test_processed = test_process(test_raw, word2index, slot2index, intent2index, LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_truly_labeled = 0\n",
    "intent_mislabeled = 0\n",
    "slot_truly_labeled = 0\n",
    "slot_mislabeled = 0\n",
    "\n",
    "for i, batch in enumerate(getBatch(BATCH_SIZE,test_processed, Shuffle = False)):\n",
    "    x,y_1,y_2 = zip(*batch)\n",
    "    x = torch.cat(x)\n",
    "    slot_target = torch.cat(y_1)\n",
    "    intent_target = torch.cat(y_2)\n",
    "    x_mask = torch.cat([Variable(torch.BoolTensor(tuple(map(lambda s: s ==0, t.data)))) for t in x]).view(BATCH_SIZE,-1)\n",
    "    y_1_mask = torch.cat([Variable(torch.BoolTensor(tuple(map(lambda s: s ==0, t.data)))) for t in slot_target]).view(BATCH_SIZE,-1)\n",
    "    \n",
    "    start_decode = Variable(torch.LongTensor([[word2index['<SOS>']]*BATCH_SIZE])).transpose(1,0)\n",
    "    \n",
    "    output, hidden_c = encoder2(x,x_mask)\n",
    "\n",
    "    slot_score, intent_score = decoder2(start_decode,hidden_c,output,x_mask)\n",
    "    \n",
    "    _,intent_predicted = torch.max(intent_score,1)\n",
    "    \n",
    "    intent_truly_labeled += sum(intent_target == intent_predicted).item()\n",
    "    intent_mislabeled += sum(intent_target != intent_predicted).item()\n",
    "    \n",
    "    _,slott_predicted = torch.max(slot_score,1)\n",
    "    \n",
    "    slot_truly_labeled += sum(slot_target.view(-1) == slot_predicted).item()\n",
    "    slot_mislabeled += sum(slot_target.view(-1) != slot_predicted).item()\n",
    "    \n",
    "    #print(slot_predicted)\n",
    "\n",
    "    #print(slot_target.view(-1))\n",
    "    #print()\n",
    "        \n",
    "intent_acc = intent_truly_labeled / (intent_truly_labeled + intent_mislabeled)\n",
    "slot_acc = slot_truly_labeled / (slot_truly_labeled + slot_mislabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8951612903225806, 0.8566196236559139)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_acc, slot_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9012096774193549, 0.6387903225806452)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_acc, slot_acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_intent_acc = intent_acc\n",
    "test_slot_acc  = slot_acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence :  i would like to find the least expensive flight from boston to denver\n",
      "Truth        :  O O O O O O B-cost_relative I-cost_relative O O B-fromloc.city_name O B-toloc.city_name\n",
      "Prediction :  O O O O O O B-cost_relative I-round_trip O O B-fromloc.city_name O B-toloc.city_name\n",
      "15\n",
      "Truth        :  atis_flight\n",
      "Prediction :  atis_flight\n"
     ]
    }
   ],
   "source": [
    "index = random.choice(range(len(test_processed)))\n",
    "\n",
    "#index = 0\n",
    "sample = test_raw[index][0]\n",
    "test_in = prepare_sequence(sample,word2index)\n",
    "\n",
    "test_mask = Variable(torch.BoolTensor(tuple(map(lambda s: s ==0, test_in.data)))).view(1,-1)\n",
    "start_decode = Variable(torch.LongTensor([[word2index['<SOS>']]*1])).transpose(1,0)\n",
    "\n",
    "output, hidden_c = encoder2(test_in.unsqueeze(0),test_mask.unsqueeze(0))\n",
    "\n",
    "slot_score, intent_score = decoder2(start_decode,hidden_c,output,test_mask)\n",
    "\n",
    "v,i = torch.max(slot_score,1)\n",
    "print(\"Input Sentence : \",*test_raw[index][0])\n",
    "print(\"Truth        : \",*test_raw[index][1])\n",
    "print(\"Prediction : \",*list(map(lambda ii:index2slot[ii],i.data.tolist())))\n",
    "v,i = torch.max(intent_score,1)\n",
    "print(i.data.tolist()[0])\n",
    "print(\"Truth        : \",test_raw[index][2])\n",
    "print(\"Prediction : \",index2intent[i.data.tolist()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_intent_acc_01 = [0.7526881720430108,\n",
    "  0.8373655913978495,\n",
    "  0.8797043010752689,\n",
    "  0.9137544802867383,\n",
    "  0.9305555555555556,\n",
    "  0.9493727598566308,\n",
    "  0.9652777777777778,\n",
    "  0.9767025089605734,\n",
    "  0.985663082437276,\n",
    "  0.9872311827956989]  # dropout = 0.1\n",
    "\n",
    "train_slot_acc_01 = [0.8645430107526881,\n",
    "  0.9174641577060932,\n",
    "  0.9322267025089606,\n",
    "  0.9536783154121864,\n",
    "  0.96331541218638,\n",
    "  0.9698611111111111,\n",
    "  0.9752912186379928,\n",
    "  0.9794758064516129,\n",
    "  0.9825313620071685,\n",
    "  0.985600358422939] # dropout = 0.1\n",
    "\n",
    "(test_intent_acc_01, test_slot_acc_01) = (0.9616935483870968, 0.8181854838709678)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9616935483870968"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x146b7ad7358>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.plot(train_intent_acc, label='train_intent_acc')\n",
    "plt.plot(train_slot_acc, label='train_slot_acc')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 19,  19,  19,  19,  67,  25,  19, 121,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  19,  19,  19,\n",
       "         19,  19,  19,  67,  19, 121,  60,  19,  19,  19,  19, 119,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,  19,  19,  19,  19,  19,  67,\n",
       "         19, 121,  19, 121,  19,  19,  19,  19,  19,  15,  19,  19,  19,  19,\n",
       "          0,   0,   0,   0,   0,  19,  19,  19,  19,  19,  67,  19, 121,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  19,  79,  19,  19,  19,  19,  67,  19, 121,  60,  19,  15,\n",
       "         19,  15,  93,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  19,\n",
       "         19,  19,  19,  19,  67,  19, 121,  60,  19,  19,  19,   3,  19,  19,\n",
       "         19,  19,   3,  19,  19,   0,   0,   0,   0,   0,  19,  19,  19,  19,\n",
       "         19,  19,  19,  19,  67,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,  19,  19,  19,  19,  67, 117,  46,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,  19,  19,  19,  19,  19,  19,  19,  67,  19, 121,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  19,  19,  19,  19,  19,  19,  19,  67,  19, 121,  19,  82,  19,\n",
       "         19,  19, 121, 117,  46,  36,   0,   0,   0,   0,   0,   0,  19,  19,\n",
       "         67,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,  79,  19,  19,  67,  25,\n",
       "         19, 121,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,  19,  19,  19,  19,  19,  79,  19,  67,\n",
       "         19,  19,  82,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,  19,  19,  19,  19,  19,  19,  19,  19,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         19,  19,  19,  19,  67,  19, 121,  19,  82,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  19,  19,  19,\n",
       "         19,  74,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slot_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 19,  19,  19,  85,  16,  19,  19,  67,  19, 121,  19,  82, 117,  46,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  19,  19,  19,\n",
       "         19,  19,  19,  19,  19,  67,  19, 121, 117,  46,  36,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,  19,  19,  19,  19,  19,  67,\n",
       "         19, 121,  19,  19,  67, 117,  46,  29,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,  19,  19,  19,  19,  19,  19,  19,  19,  54,\n",
       "         19,  74,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  19,  19,  19,  19,  19,  19,   8,  10,  19,  67,  19, 121,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  19,\n",
       "         19,  19,  19,  67,  19, 121, 117,  46,  36,  36,  19,  82,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  19,  19,  19,  32,\n",
       "         19,  19,  67,  19, 121,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,  19,  19,  19,  19,  19,  79,  19,\n",
       "         67,  19,  19,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,  19,  19,  19,  67,  25,  25,  19, 121,  60,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  19,  19,  19,  19,  19,  19,  19,  67,  19, 121,  19,  19,  19,\n",
       "         19,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  19,  19,\n",
       "         19,  19,  19,  67,  19, 121,  19,  47, 113,  31,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,  19,  19,  19,  19,  19,\n",
       "        113,  31,  19,  67,  19, 121,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,  19,  19,  19,  19,  19,  19,  19,  19,\n",
       "         19,  19,  67,  19,  19, 121,  19,  19,  21,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,  19,  19,  19,  19,  19,  67,  19, 121,  19,  82,  19,\n",
       "         53,  19,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         19,  19,  19,  19,  19,  79, 107,  19,  67,  19, 121,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  19,  67,\n",
       "         19, 121,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slot_target.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
